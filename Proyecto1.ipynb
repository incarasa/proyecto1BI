{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17dc9b3e-1537-4c47-a1c7-659a6ca32ebd",
   "metadata": {},
   "source": [
    "# Avance 1: Proyecto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113470f4-f2ea-4032-8593-9ca9003a697d",
   "metadata": {},
   "source": [
    "## Entendimiento y preparación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5845ad7d-b8b6-42b2-a950-6f1282554260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0908f44-b640-4a45-baa6-ee9bc8e81e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textos</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Aprendizaje\" y \"educación\" se consideran sinó...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Para los niños más pequeños (bebés y niños peq...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Además, la formación de especialistas en medic...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>En los países de la OCDE se tiende a pasar de ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Este grupo se centró en las personas que padec...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              textos  labels\n",
       "0  \"Aprendizaje\" y \"educación\" se consideran sinó...       4\n",
       "1  Para los niños más pequeños (bebés y niños peq...       4\n",
       "2  Además, la formación de especialistas en medic...       3\n",
       "3  En los países de la OCDE se tiende a pasar de ...       4\n",
       "4  Este grupo se centró en las personas que padec...       3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ruta del archivo\n",
    "file_path = \"Datos_proyecto.xlsx\"\n",
    "\n",
    "# Cargar el Excel\n",
    "df = pd.read_excel(file_path, sheet_name=\"Sheet1\")\n",
    "\n",
    "# Mostrar las primeras filas\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c745257a-6827-459c-9579-06bb1a47ef5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2424 entries, 0 to 2423\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   textos  2424 non-null   object\n",
      " 1   labels  2424 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 38.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# Ver información general\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8d5352-1f11-4c1f-86f3-a91de8ee52fa",
   "metadata": {},
   "source": [
    "Separamos nuestros conjuntos de entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ca85c6a-9b85-42fc-b937-d4a8910ed82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"textos\"], df[\"labels\"], test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39a5275-1fa5-42d2-a4e1-467db269fc3e",
   "metadata": {},
   "source": [
    "Ahora empezaremos construyendo el pipeline de preprocesamiento y entrenamiento de nuestros datos. Pero antes revisaremos TfidfVectorizer que nos ayudará a preprocesar nuestras entradas y a construir nuestra bag of words (BOW)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0cf88ce2-0259-4e0b-86d0-fdf730fe003f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nicop\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(306,\n",
       " ['a',\n",
       "  'al',\n",
       "  'algo',\n",
       "  'algunas',\n",
       "  'algunos',\n",
       "  'ante',\n",
       "  'antes',\n",
       "  'como',\n",
       "  'con',\n",
       "  'contra',\n",
       "  'cual',\n",
       "  'cuando',\n",
       "  'de',\n",
       "  'del',\n",
       "  'desde',\n",
       "  'donde',\n",
       "  'durante',\n",
       "  'e',\n",
       "  'el',\n",
       "  'ella'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Se preparan las stopwords en español para utilizar en vectorizer\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Se quitan los acentos de las stopwords, lo cual también es importante (evitar warnings)\n",
    "def strip_accents(s: str) -> str:\n",
    "    return ''.join(c for c in unicodedata.normalize('NFKD', s)\n",
    "                   if not unicodedata.combining(c))\n",
    "\n",
    "\n",
    "spanish_stopwords = sorted({ strip_accents(w.lower()) for w in stopwords.words('spanish') })\n",
    "len(spanish_stopwords), spanish_stopwords[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a626099b-f377-4079-a736-ae46781a6fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras en el vocabulario:\n",
      " ['cada' 'cada vez' 'forma' 'formacion' 'mental' 'mentales' 'nivel' 'ocde'\n",
      " 'pueden' 'salud' 'salud mental' 'servicios' 'servicios salud'\n",
      " 'trastornos' 'trastornos mentales' 'tratamiento' 'vez']\n"
     ]
    }
   ],
   "source": [
    "# Usamos solo una muestra pequeña de tus datos para verlos \"a mano\"\n",
    "muestra = df[\"textos\"].head(5)\n",
    "\n",
    "# Crear el vectorizador\n",
    "vectorizer = TfidfVectorizer(\n",
    "            stop_words=spanish_stopwords,   # stop words en español\n",
    "            lowercase=True,                 # minúsculas\n",
    "            strip_accents='unicode',      # quitar los acentos: educación -> educacion\n",
    "            min_df=2,                       # ignora términos que aparezcan en <2 docs\n",
    "            max_df=0.9,                     # ignora términos muy frecuentes >90%\n",
    "            ngram_range=(1,2),              # se incluyen bigramas\n",
    "            sublinear_tf=True,\n",
    "            max_features=20000,           # evita sobrecargas por bigramas\n",
    ")\n",
    "\n",
    "# Ajustar y transformar\n",
    "X_tfidf = vectorizer.fit_transform(muestra)\n",
    "\n",
    "# Ver las palabras del vocabulario que creó TF-IDF\n",
    "print(\"Palabras en el vocabulario:\\n\", vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891305f2-b4f7-46d5-9610-6c400cfaae77",
   "metadata": {},
   "source": [
    "## Modelado y Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9875812a-a17b-4bae-9f11-e238380e9770",
   "metadata": {},
   "source": [
    "Ahora construiremos los pipelines para el preprocesamiento y entrenamiento de nuestros datos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f1215c6-e8f1-4345-b6b0-6e0e4ff6ca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    stop_words=sorted(spanish_stopwords),\n",
    "    lowercase=True,\n",
    "    strip_accents='unicode',\n",
    "    min_df=2,          # ignora términos muy raros\n",
    "    max_df=0.9,        # ignora términos demasiado frecuentes\n",
    "    ngram_range=(1,2), # unigrams + bigrams\n",
    "    sublinear_tf=True, # tf = 1 + log(tf) AJUSTE PARA EL CONTEO, MEJOR LOGARITMICO\n",
    "    max_features=20000 # límite para no explotar RAM\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da88b59f-3c1c-4904-ab33-2ab29737dd04",
   "metadata": {},
   "source": [
    "**Pipeline 1 (Regresión Logística)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c22933f-32a8-4a51-aaca-b843ac2ee131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\profiling\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1272: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Regresión Logística ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.956     0.964     0.960       112\n",
      "           3      0.971     0.982     0.976       168\n",
      "           4      0.995     0.980     0.988       205\n",
      "\n",
      "    accuracy                          0.977       485\n",
      "   macro avg      0.974     0.976     0.975       485\n",
      "weighted avg      0.978     0.977     0.977       485\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Pipeline: TF-IDF + Logistic Regression\n",
    "pipe_logreg = Pipeline([\n",
    "    (\"tfidf\", tfidf),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        max_iter=2000,\n",
    "        class_weight=\"balanced\",  # maneja desbalance de clases\n",
    "        multi_class=\"auto\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipe_logreg.fit(X_train, y_train)\n",
    "y_pred_logreg = pipe_logreg.predict(X_test)\n",
    "\n",
    "print(\"=== Regresión Logística ===\")\n",
    "print(classification_report(y_test, y_pred_logreg, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d447bc94-a1c5-49d6-94af-a2ed75d83bdb",
   "metadata": {},
   "source": [
    "**Pipeline 2 (SVM Máquinas de Vectores de Soporte)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "796c8eac-70ec-452a-98ed-41bab109bfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Support Vector Machine (Lineal) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.956     0.964     0.960       112\n",
      "           3      0.965     0.982     0.973       168\n",
      "           4      0.995     0.976     0.985       205\n",
      "\n",
      "    accuracy                          0.975       485\n",
      "   macro avg      0.972     0.974     0.973       485\n",
      "weighted avg      0.976     0.975     0.975       485\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Pipeline: TF-IDF + Linear SVM\n",
    "pipe_svm = Pipeline([\n",
    "    (\"tfidf\", tfidf),\n",
    "    (\"clf\", LinearSVC(class_weight=\"balanced\", dual=True))\n",
    "])\n",
    "\n",
    "pipe_svm.fit(X_train, y_train)\n",
    "y_pred_svm = pipe_svm.predict(X_test)\n",
    "\n",
    "print(\"=== Support Vector Machine (Lineal) ===\")\n",
    "print(classification_report(y_test, y_pred_svm, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a349ff35-a7fd-410c-9fda-5b549df60c42",
   "metadata": {},
   "source": [
    "**Pipeline 3 (Bayes Ingenuo Multinomial)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7fb3392c-5bd9-4995-8df9-66fb3b8e81a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Naive Bayes Multinomial ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.969     0.830     0.894       112\n",
      "           3      0.943     0.976     0.959       168\n",
      "           4      0.940     0.985     0.962       205\n",
      "\n",
      "    accuracy                          0.946       485\n",
      "   macro avg      0.950     0.931     0.938       485\n",
      "weighted avg      0.947     0.946     0.945       485\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Pipeline: TF-IDF + Multinomial Naive Bayes\n",
    "pipe_nb = Pipeline([\n",
    "    (\"tfidf\", tfidf),\n",
    "    (\"clf\", MultinomialNB(alpha=0.5))  # alpha suaviza probabilidades (HIPERPARAMETRO?)\n",
    "])\n",
    "\n",
    "pipe_nb.fit(X_train, y_train)\n",
    "y_pred_nb = pipe_nb.predict(X_test)\n",
    "\n",
    "print(\"=== Naive Bayes Multinomial ===\")\n",
    "print(classification_report(y_test, y_pred_nb, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bc8cd2",
   "metadata": {},
   "source": [
    "**Optimización de Hiperparámetros – Pipeline 3 (Multinomial Naive Bayes)**\n",
    "\n",
    "Con el fin de mejorar el rendimiento del clasificador **Multinomial Naive Bayes** aplicado sobre representaciones TF-IDF, se implementó un proceso de búsqueda exhaustiva de hiperparámetros mediante **GridSearchCV** con validación cruzada estratificada (5 folds).  \n",
    "\n",
    "##### Parámetros evaluados\n",
    "- **Vectorización TF-IDF**\n",
    "  - `ngram_range`: (1,1) y (1,2) → solo unigramas vs. unigramas + bigramas  \n",
    "  - `min_df`: [1, 2, 5] → frecuencia mínima para incluir términos  \n",
    "  - `max_df`: [0.85, 0.95] → frecuencia máxima para descartar términos muy comunes  \n",
    "  - `sublinear_tf`: [True] → transformación log(1+tf) para estabilizar pesos  \n",
    "  - `strip_accents`: ['unicode'] → normalización de acentos  \n",
    "  - `stop_words`: [None, spanish_stopwords] → sin filtrado vs. lista de stopwords en español\n",
    "\n",
    "- **Selección de características**\n",
    "  - `SelectKBest(chi2, k=5000/10000)` → mantuvo solo los términos más informativos  \n",
    "  - `\"passthrough\"` → sin selección, se mantienen todas las features  \n",
    "\n",
    "- **Clasificador MultinomialNB**\n",
    "  - `alpha`: [0.1, 0.5, 1.0, 2.0] → suavizado de Laplace para evitar probabilidades cero  \n",
    "  - `fit_prior`: [True, False] → usar priors ajustados a la frecuencia de clases vs. priors uniformes  \n",
    "\n",
    "##### Métrica de evaluación\n",
    "Se utilizó **F1-macro** como métrica principal, ya que promedia equitativamente el rendimiento en todas las clases, evitando sesgos hacia la clase mayoritaria.  \n",
    "\n",
    "##### Mejores parámetros encontrados\n",
    "```python\n",
    "{\n",
    " 'clf__alpha': 1.0,\n",
    " 'clf__fit_prior': False,\n",
    " 'select': SelectKBest(chi2, k=5000),\n",
    " 'tfidf__max_df': 0.85,\n",
    " 'tfidf__min_df': 2,\n",
    " 'tfidf__ngram_range': (1, 2),\n",
    " 'tfidf__stop_words': None,\n",
    " 'tfidf__strip_accents': 'unicode',\n",
    " 'tfidf__sublinear_tf': True\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a754b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 576 candidates, totalling 2880 fits\n",
      "Mejores hiperparámetros encontrados:\n",
      "{'clf__alpha': 1.0, 'clf__fit_prior': False, 'select': SelectKBest(k=5000, score_func=<function chi2 at 0x000001F52F43DBD0>), 'tfidf__max_df': 0.85, 'tfidf__min_df': 2, 'tfidf__ngram_range': (1, 2), 'tfidf__stop_words': None, 'tfidf__strip_accents': 'unicode', 'tfidf__sublinear_tf': True}\n",
      "Mejor F1_macro (CV): 0.9707\n",
      "\n",
      "==> TEST (MNB mejorado)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.9464    0.9464    0.9464       112\n",
      "           3     0.9760    0.9702    0.9731       168\n",
      "           4     0.9757    0.9805    0.9781       205\n",
      "\n",
      "    accuracy                         0.9691       485\n",
      "   macro avg     0.9661    0.9657    0.9659       485\n",
      "weighted avg     0.9691    0.9691    0.9691       485\n",
      "\n",
      "F1_macro test: 0.9658883631892673\n",
      "Matriz de confusión:\n",
      " [[106   2   4]\n",
      " [  4 163   1]\n",
      " [  2   2 201]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "pipe_nb = Pipeline(steps=[\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"select\", \"passthrough\"),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "# Vmos a definir un grid de hiperparámetros para buscar la mejor configuración\n",
    "# Usamos SelectKBest con χ² para selección de características (o no)\n",
    "\n",
    "param_grid_nb = {\n",
    "    # TF-IDF: mejorar representación\n",
    "    \"tfidf__ngram_range\": [(1,1), (1,2)],             # Queremos unigramas y bigramas porque hay terminos u frases claves en pares\n",
    "    \"tfidf__min_df\": [1, 2, 5],                       # filtra términos muy raros\n",
    "    \"tfidf__max_df\": [0.85, 0.95],                    # filtra términos muy frecuentes\n",
    "    \"tfidf__sublinear_tf\": [True],                    # Transformar TF a log(1+tf), reduce peso de términos muy frecuentes\n",
    "    \"tfidf__strip_accents\": [\"unicode\"],              # normaliza acentos\n",
    "    \"tfidf__stop_words\": [None, spanish_stopwords], # stop words en español\n",
    "\n",
    "    # Selección de características χ² (o no)\n",
    "    \"select\": [SelectKBest(chi2, k=5000),\n",
    "               SelectKBest(chi2, k=10000),\n",
    "               \"passthrough\"],\n",
    "\n",
    "    # NB: suavizado y priors\n",
    "    \"clf__alpha\": [0.1, 0.5, 1.0, 2.0],               # suavizado de Laplace: “simula” como si todas las palabras aparecieran al menos una vez.\n",
    "                                                      # Evita probabilidades cero para palabras no vistas en una clase.\n",
    "\n",
    "    \"clf__fit_prior\": [True, False],                  # usar priors aprendidos vs uniformes\n",
    "                                                      # En clases desbalanceadas, priors aprendidos suelen ayudar\n",
    "}\n",
    "\n",
    "\n",
    "# Se divide el conjunto de entrenamiento en 5 folds estratificados\n",
    "# para mantener la proporción de clases en cada fold\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "gs_nb = GridSearchCV(\n",
    "    estimator=pipe_nb, # el pipeline base a optimizar\n",
    "    param_grid=param_grid_nb, # el grid de hiperparámetros\n",
    "    scoring=\"f1_macro\", \n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "gs_nb.fit(X_train, y_train)\n",
    "\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(gs_nb.best_params_)\n",
    "print(f\"Mejor F1_macro (CV): {gs_nb.best_score_:.4f}\")\n",
    "\n",
    "# Evaluación en test\n",
    "y_pred = gs_nb.predict(X_test)\n",
    "print(\"\\n==> TEST (MNB mejorado)\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "print(\"F1_macro test:\", f1_score(y_test, y_pred, average=\"macro\"))\n",
    "print(\"Matriz de confusión:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d79c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Resultados Pipeline 3 (NB Final)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.9464    0.9464    0.9464       112\n",
      "           3     0.9760    0.9702    0.9731       168\n",
      "           4     0.9757    0.9805    0.9781       205\n",
      "\n",
      "    accuracy                         0.9691       485\n",
      "   macro avg     0.9661    0.9657    0.9659       485\n",
      "weighted avg     0.9691    0.9691    0.9691       485\n",
      "\n",
      "F1_macro test: 0.9658883631892673\n",
      "Matriz de confusión:\n",
      " [[106   2   4]\n",
      " [  4 163   1]\n",
      " [  2   2 201]]\n"
     ]
    }
   ],
   "source": [
    "# Pipeline 3 – Multinomial Naive Bayes (versión con mejores parámetros)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "# Definir el pipeline con los mejores hiperparámetros\n",
    "nb_final = Pipeline(steps=[\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        max_df=0.85,\n",
    "        sublinear_tf=True,\n",
    "        strip_accents=\"unicode\",\n",
    "        stop_words=None\n",
    "    )),\n",
    "    (\"select\", SelectKBest(score_func=chi2, k=5000)),\n",
    "    (\"clf\", MultinomialNB(alpha=1.0, fit_prior=False))\n",
    "])\n",
    "\n",
    "# Entrenar\n",
    "nb_final.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones en test\n",
    "y_pred = nb_final.predict(X_test)\n",
    "\n",
    "# Evaluación\n",
    "print(\"==> Resultados Pipeline 3\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "print(\"F1_macro test:\", f1_score(y_test, y_pred, average=\"macro\"))\n",
    "print(\"Matriz de confusión:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8080d14d",
   "metadata": {},
   "source": [
    "#### Celda: Pipeline Naive Bayes con combinación de word n-grams y char n-grams\n",
    "\n",
    "En esta celda se implementó un **nuevo pipeline** para el clasificador Naive Bayes, \n",
    "enfocado en enriquecer la representación de los textos mediante la combinación de \n",
    "**n-gramas de palabras** y **n-gramas de caracteres**.\n",
    "\n",
    "##### ¿Qué se hace?\n",
    "1. **Vectorización doble del texto**:\n",
    "   - **Word n-grams (1,2)**: genera unigramas y bigramas para capturar tanto palabras sueltas \n",
    "     como expresiones clave (*“salud pública”*, *“educación calidad”*).  \n",
    "   - **Char n-grams (3–5)**: genera secuencias de 3 a 5 caracteres para capturar \n",
    "     sufijos/prefijos (*“-ción”*, *“edu-”*), variaciones morfológicas y errores de escritura.  \n",
    "2. **Unión de representaciones**: ambas salidas TF-IDF se combinan con `FeatureUnion`, \n",
    "   creando un espacio de características más rico.  \n",
    "3. **Selección de características**: se usa `SelectKBest(chi2, k=10000)` para quedarse con \n",
    "   las 10.000 más informativas y reducir ruido.  \n",
    "4. **Clasificación**: se entrena un **Multinomial Naive Bayes** con `alpha=1.0` y \n",
    "   `fit_prior=False` (priors uniformes para balancear clases).\n",
    "\n",
    "##### ¿Qué se busca?\n",
    "- **Mejorar la robustez** frente a errores ortográficos o variaciones en las palabras.  \n",
    "- **Capturar contexto semántico y morfológico** al mismo tiempo, combinando niveles \n",
    "  diferentes de análisis (palabra y carácter).  \n",
    "- **Aumentar el F1-macro** y lograr un modelo más equilibrado en la clasificación de ODS.\n",
    "\n",
    "##### ¿Por qué?\n",
    "- Los **word n-grams** son muy buenos para detectar expresiones directamente relacionadas \n",
    "  con los ODS, pero pueden fallar si hay errores de escritura.  \n",
    "- Los **char n-grams** complementan al modelo al detectar patrones sub-léxicos que son \n",
    "  comunes incluso con variaciones (ej. “educacion” vs “educación”).  \n",
    "- La combinación aporta una señal más completa, y la selección de características \n",
    "  evita que el modelo se vea afectado por ruido excesivo.\n",
    "\n",
    "##### Resultado esperado\n",
    "Con esta celda se consiguió un rendimiento superior al pipeline anterior:  \n",
    "- **F1-macro en test = 0.9735** (vs 0.9659 del mejor pipeline anterior).  \n",
    "- Matriz de confusión más limpia y mejor balance entre las tres clases.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3d3213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> NB word+char\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.9550    0.9464    0.9507       112\n",
      "           3     0.9880    0.9762    0.9820       168\n",
      "           4     0.9808    0.9951    0.9879       205\n",
      "\n",
      "    accuracy                         0.9773       485\n",
      "   macro avg     0.9746    0.9726    0.9735       485\n",
      "weighted avg     0.9773    0.9773    0.9773       485\n",
      "\n",
      "F1_macro test: 0.9735340121177855\n",
      "Matriz de confusión:\n",
      " [[106   2   4]\n",
      " [  4 164   0]\n",
      " [  1   0 204]]\n"
     ]
    }
   ],
   "source": [
    "# Pipeline NB con mezcla de word n-grams (1-2) + char n-grams (3-5)\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "\n",
    "# Bloque de features: une dos TF-IDF sobre el MISMO texto (word + char)\n",
    "features_union = FeatureUnion(transformer_list=[\n",
    "    (\"tfidf_word\", TfidfVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        max_df=0.85,\n",
    "        sublinear_tf=True,\n",
    "        strip_accents=\"unicode\"\n",
    "        stop_words=None\n",
    "    )),\n",
    "    (\"tfidf_char\", TfidfVectorizer(\n",
    "        analyzer=\"char\",\n",
    "        ngram_range=(3, 5),\n",
    "        min_df=2,\n",
    "        sublinear_tf=True\n",
    "        # max_df por defecto (=1.0) suele ir bien para char n-grams\n",
    "    )),\n",
    "])\n",
    "\n",
    "nb_charword_final = Pipeline(steps=[\n",
    "    (\"vec\", features_union),\n",
    "    (\"select\", SelectKBest(score_func=chi2, k=10000)),   # puedes ajustar k\n",
    "    (\"clf\", MultinomialNB(alpha=1.0, fit_prior=False))\n",
    "])\n",
    "\n",
    "# Entrenar\n",
    "nb_charword_final.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar\n",
    "y_pred_cw = nb_charword_final.predict(X_test)\n",
    "print(\"==> NB word+char\")\n",
    "print(classification_report(y_test, y_pred_cw, digits=4))\n",
    "print(\"F1_macro test:\", f1_score(y_test, y_pred_cw, average=\"macro\"))\n",
    "print(\"Matriz de confusión:\\n\", confusion_matrix(y_test, y_pred_cw))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61ee9be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Resultados Pipeline 3 Final (NB Word+Char Optimizado)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.9550    0.9464    0.9507       112\n",
      "           3     0.9880    0.9762    0.9820       168\n",
      "           4     0.9808    0.9951    0.9879       205\n",
      "\n",
      "    accuracy                         0.9773       485\n",
      "   macro avg     0.9746    0.9726    0.9735       485\n",
      "weighted avg     0.9773    0.9773    0.9773       485\n",
      "\n",
      "F1_macro test: 0.9735340121177855\n",
      "Matriz de confusión:\n",
      " [[106   2   4]\n",
      " [  4 164   0]\n",
      " [  1   0 204]]\n"
     ]
    }
   ],
   "source": [
    "# Pipeline 3 Final – Multinomial Naive Bayes optimizado (word + char n-grams)\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "\n",
    "# Vectorización combinada\n",
    "features_union = FeatureUnion(transformer_list=[\n",
    "    (\"tfidf_word\", TfidfVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        max_df=0.85,\n",
    "        sublinear_tf=True,\n",
    "        strip_accents=\"unicode\",\n",
    "        stop_words=None  # No usar stop words para capturar más patrones\n",
    "    )),\n",
    "    (\"tfidf_char\", TfidfVectorizer(\n",
    "        analyzer=\"char\",\n",
    "        ngram_range=(3, 5),\n",
    "        min_df=2,\n",
    "        sublinear_tf=True\n",
    "    )),\n",
    "])\n",
    "\n",
    "# Pipeline final\n",
    "nb_pipeline_final = Pipeline(steps=[\n",
    "    (\"vec\", features_union),\n",
    "    (\"select\", SelectKBest(score_func=chi2, k=10000)),\n",
    "    (\"clf\", MultinomialNB(alpha=1.0, fit_prior=False))\n",
    "])\n",
    "\n",
    "# Entrenar\n",
    "nb_pipeline_final.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones en test\n",
    "y_pred_final = nb_pipeline_final.predict(X_test)\n",
    "\n",
    "# Evaluación\n",
    "print(\"==> Resultados Pipeline 3 Final (NB Word+Char Optimizado)\")\n",
    "print(classification_report(y_test, y_pred_final, digits=4))\n",
    "print(\"F1_macro test:\", f1_score(y_test, y_pred_final, average=\"macro\"))\n",
    "print(\"Matriz de confusión:\\n\", confusion_matrix(y_test, y_pred_final))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8edac6c",
   "metadata": {},
   "source": [
    "## Analisis de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64ea75cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\profiling\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1281: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "c:\\ProgramData\\anaconda3\\envs\\profiling\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Regresión Logística ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.9901    0.8929    0.9390       112\n",
      "           3     0.9489    0.9940    0.9709       168\n",
      "           4     0.9808    0.9951    0.9879       205\n",
      "\n",
      "    accuracy                         0.9711       485\n",
      "   macro avg     0.9732    0.9607    0.9659       485\n",
      "weighted avg     0.9719    0.9711    0.9707       485\n",
      "\n",
      "Matriz de confusión:\n",
      " [[100   8   4]\n",
      " [  1 167   0]\n",
      " [  0   1 204]]\n",
      "\n",
      "==================== SVM lineal ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.9640    0.9554    0.9596       112\n",
      "           3     0.9708    0.9881    0.9794       168\n",
      "           4     0.9901    0.9805    0.9853       205\n",
      "\n",
      "    accuracy                         0.9773       485\n",
      "   macro avg     0.9750    0.9746    0.9748       485\n",
      "weighted avg     0.9774    0.9773    0.9773       485\n",
      "\n",
      "Matriz de confusión:\n",
      " [[107   3   2]\n",
      " [  2 166   0]\n",
      " [  2   2 201]]\n",
      "\n",
      "==================== Naive Bayes (word+char + χ²) ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.9550    0.9464    0.9507       112\n",
      "           3     0.9880    0.9762    0.9820       168\n",
      "           4     0.9808    0.9951    0.9879       205\n",
      "\n",
      "    accuracy                         0.9773       485\n",
      "   macro avg     0.9746    0.9726    0.9735       485\n",
      "weighted avg     0.9773    0.9773    0.9773       485\n",
      "\n",
      "Matriz de confusión:\n",
      " [[106   2   4]\n",
      " [  4 164   0]\n",
      " [  1   0 204]]\n",
      "\n",
      "==> Comparación de modelos (test):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelo</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>precision_weighted</th>\n",
       "      <th>recall_weighted</th>\n",
       "      <th>f1_weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM lineal</td>\n",
       "      <td>0.977320</td>\n",
       "      <td>0.974957</td>\n",
       "      <td>0.974647</td>\n",
       "      <td>0.974762</td>\n",
       "      <td>0.977386</td>\n",
       "      <td>0.977320</td>\n",
       "      <td>0.977312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Naive Bayes (word+char + χ²)</td>\n",
       "      <td>0.977320</td>\n",
       "      <td>0.974559</td>\n",
       "      <td>0.972580</td>\n",
       "      <td>0.973534</td>\n",
       "      <td>0.977296</td>\n",
       "      <td>0.977320</td>\n",
       "      <td>0.977269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Regresión Logística</td>\n",
       "      <td>0.971134</td>\n",
       "      <td>0.973244</td>\n",
       "      <td>0.960676</td>\n",
       "      <td>0.965930</td>\n",
       "      <td>0.971872</td>\n",
       "      <td>0.971134</td>\n",
       "      <td>0.970719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         modelo  accuracy  precision_macro  recall_macro  \\\n",
       "0                    SVM lineal  0.977320         0.974957      0.974647   \n",
       "1  Naive Bayes (word+char + χ²)  0.977320         0.974559      0.972580   \n",
       "2           Regresión Logística  0.971134         0.973244      0.960676   \n",
       "\n",
       "   f1_macro  precision_weighted  recall_weighted  f1_weighted  \n",
       "0  0.974762            0.977386         0.977320     0.977312  \n",
       "1  0.973534            0.977296         0.977320     0.977269  \n",
       "2  0.965930            0.971872         0.971134     0.970719  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Comparación final: Regresión Logística vs SVM lineal vs NB (word+char + χ²) ===\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    accuracy_score, precision_recall_fscore_support\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "# --- Representación: word n-grams (1–2) + char n-grams (3–5) ---\n",
    "features_union = FeatureUnion(transformer_list=[\n",
    "    (\"tfidf_word\", TfidfVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        max_df=0.85,\n",
    "        sublinear_tf=True,\n",
    "        strip_accents=\"unicode\",\n",
    "        stop_words=None\n",
    "    )),\n",
    "    (\"tfidf_char\", TfidfVectorizer(\n",
    "        analyzer=\"char\",\n",
    "        ngram_range=(3, 5),\n",
    "        min_df=2,\n",
    "        sublinear_tf=True\n",
    "    )),\n",
    "])\n",
    "\n",
    "# --- Pipelines de los 3 modelos ---\n",
    "pipe_logreg = Pipeline(steps=[\n",
    "    (\"vec\", features_union),\n",
    "    (\"select\", SelectKBest(score_func=chi2, k=10000)),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        solver=\"liblinear\",      \n",
    "        multi_class=\"ovr\",\n",
    "        max_iter=2000\n",
    "        \n",
    "    ))\n",
    "])\n",
    "\n",
    "pipe_svm = Pipeline(steps=[\n",
    "    (\"vec\", features_union),\n",
    "    (\"select\", SelectKBest(score_func=chi2, k=10000)),\n",
    "    (\"clf\", LinearSVC(C=1.0))    # SVM lineal (rápida en BoW/TF-IDF)\n",
    "])\n",
    "\n",
    "pipe_nb_final = Pipeline(steps=[\n",
    "    (\"vec\", features_union),\n",
    "    (\"select\", SelectKBest(score_func=chi2, k=10000)),\n",
    "    (\"clf\", MultinomialNB(alpha=1.0, fit_prior=False))\n",
    "])\n",
    "\n",
    "# --- Función de evaluación y resumen ---\n",
    "def evaluar_y_resumir(nombre, clf, X_train, y_train, X_test, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(f\"\\n{'='*20} {nombre} {'='*20}\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "    print(\"Matriz de confusión:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    p_macro, r_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    p_weight, r_weight, f1_weight, _ = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average=\"weighted\", zero_division=0\n",
    "    )\n",
    "    return {\n",
    "        \"modelo\": nombre,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision_macro\": p_macro,\n",
    "        \"recall_macro\": r_macro,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"precision_weighted\": p_weight,\n",
    "        \"recall_weighted\": r_weight,\n",
    "        \"f1_weighted\": f1_weight,\n",
    "    }\n",
    "\n",
    "# --- Correr y comparar ---\n",
    "resultados = []\n",
    "for nombre, modelo in [\n",
    "    (\"Regresión Logística\", pipe_logreg),\n",
    "    (\"SVM lineal\", pipe_svm),\n",
    "    (\"Naive Bayes (word+char + χ²)\", pipe_nb_final),\n",
    "]:\n",
    "    resultados.append(evaluar_y_resumir(nombre, modelo, X_train, y_train, X_test, y_test))\n",
    "\n",
    "df_comp = pd.DataFrame(resultados).sort_values(by=\"f1_macro\", ascending=False).reset_index(drop=True)\n",
    "print(\"\\n==> Comparación de modelos (test):\")\n",
    "display(df_comp)  # en Jupyter; si no, usa print(df_comp.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5914709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Comparación de modelos (test)\n",
      "                      modelo  accuracy  precision_macro  recall_macro  f1_macro  precision_weighted  recall_weighted  f1_weighted\n",
      "                  SVM lineal  0.977320         0.974957      0.974647  0.974762            0.977386         0.977320     0.977312\n",
      "Naive Bayes (word+char + χ²)  0.977320         0.974559      0.972580  0.973534            0.977296         0.977320     0.977269\n",
      "         Regresión Logística  0.971134         0.973244      0.960676  0.965930            0.971872         0.971134     0.970719\n",
      "\n",
      "Archivo Excel guardado con openpyxl: comparacion_modelos_LR_SVM_NB.xlsx\n"
     ]
    }
   ],
   "source": [
    "# === Exportar comparación a Excel\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "# 1) Recalcular/asegurar df_comp, reportes y matrices\n",
    "modelos = [\n",
    "    (\"SVM lineal\",                      pipe_svm),\n",
    "    (\"Naive Bayes (word+char + χ²)\",    pipe_nb_final),\n",
    "    (\"Regresión Logística\",             pipe_logreg),\n",
    "]\n",
    "\n",
    "filas, reportes, cms = [], {}, {}\n",
    "etiquetas = sorted(set(y_test))\n",
    "\n",
    "for nombre, clf in modelos:\n",
    "    y_pred = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    p_m, r_m, f1_m, _ = precision_recall_fscore_support(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "    p_w, r_w, f1_w, _ = precision_recall_fscore_support(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "    filas.append({\n",
    "        \"modelo\": nombre,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision_macro\": p_m,\n",
    "        \"recall_macro\": r_m,\n",
    "        \"f1_macro\": f1_m,\n",
    "        \"precision_weighted\": p_w,\n",
    "        \"recall_weighted\": r_w,\n",
    "        \"f1_weighted\": f1_w,\n",
    "    })\n",
    "    reportes[nombre] = pd.DataFrame(classification_report(y_test, y_pred, output_dict=True)).T\n",
    "    cms[nombre] = pd.DataFrame(\n",
    "        confusion_matrix(y_test, y_pred, labels=etiquetas),\n",
    "        index=[f\"true_{c}\" for c in etiquetas],\n",
    "        columns=[f\"pred_{c}\" for c in etiquetas]\n",
    "    )\n",
    "\n",
    "df_comp = pd.DataFrame(filas).sort_values(\"f1_macro\", ascending=False).reset_index(drop=True)\n",
    "print(\"==> Comparación de modelos (test)\")\n",
    "print(df_comp.to_string(index=False))\n",
    "\n",
    "# 2) Exportar a Excel\n",
    "excel_path = \"comparacion_modelos_LR_SVM_NB.xlsx\"\n",
    "\n",
    "def exportar_a_excel(path, resumen, reps, mats):\n",
    "    try:\n",
    "        # Primer intento: openpyxl (suele venir en Anaconda)\n",
    "        with pd.ExcelWriter(path, engine=\"openpyxl\") as writer:\n",
    "            resumen.to_excel(writer, sheet_name=\"resumen\", index=False)\n",
    "            for nombre, rep in reps.items():\n",
    "                rep.to_excel(writer, sheet_name=f\"reporte_{nombre[:20]}\", index=True)\n",
    "            for nombre, cm in mats.items():\n",
    "                cm.to_excel(writer, sheet_name=f\"cm_{nombre[:20]}\", index=True)\n",
    "        print(f\"\\nArchivo Excel guardado con openpyxl: {path}\")\n",
    "        return True\n",
    "    except ModuleNotFoundError:\n",
    "        try:\n",
    "            # Segundo intento: xlsxwriter\n",
    "            with pd.ExcelWriter(path, engine=\"xlsxwriter\") as writer:\n",
    "                resumen.to_excel(writer, sheet_name=\"resumen\", index=False)\n",
    "                for nombre, rep in reps.items():\n",
    "                    rep.to_excel(writer, sheet_name=f\"reporte_{nombre[:20]}\", index=True)\n",
    "                for nombre, cm in mats.items():\n",
    "                    cm.to_excel(writer, sheet_name=f\"cm_{nombre[:20]}\", index=True)\n",
    "            print(f\"\\nArchivo Excel guardado con xlsxwriter: {path}\")\n",
    "            return True\n",
    "        except ModuleNotFoundError:\n",
    "            return False\n",
    "\n",
    "ok = exportar_a_excel(excel_path, df_comp, reportes, cms)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "profiling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
