{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17dc9b3e-1537-4c47-a1c7-659a6ca32ebd",
   "metadata": {},
   "source": [
    "# Avance 1: Proyecto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113470f4-f2ea-4032-8593-9ca9003a697d",
   "metadata": {},
   "source": [
    "## Entendimiento y preparación de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5845ad7d-b8b6-42b2-a950-6f1282554260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0908f44-b640-4a45-baa6-ee9bc8e81e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textos</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Aprendizaje\" y \"educación\" se consideran sinó...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Para los niños más pequeños (bebés y niños peq...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Además, la formación de especialistas en medic...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>En los países de la OCDE se tiende a pasar de ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Este grupo se centró en las personas que padec...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              textos  labels\n",
       "0  \"Aprendizaje\" y \"educación\" se consideran sinó...       4\n",
       "1  Para los niños más pequeños (bebés y niños peq...       4\n",
       "2  Además, la formación de especialistas en medic...       3\n",
       "3  En los países de la OCDE se tiende a pasar de ...       4\n",
       "4  Este grupo se centró en las personas que padec...       3"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ruta del archivo\n",
    "file_path = \"Datos_proyecto.xlsx\"\n",
    "\n",
    "# Cargar el Excel\n",
    "df = pd.read_excel(file_path, sheet_name=\"Sheet1\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c745257a-6827-459c-9579-06bb1a47ef5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2424 entries, 0 to 2423\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   textos  2424 non-null   object\n",
      " 1   labels  2424 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 38.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8d5352-1f11-4c1f-86f3-a91de8ee52fa",
   "metadata": {},
   "source": [
    "Separamos nuestros conjuntos de entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ca85c6a-9b85-42fc-b937-d4a8910ed82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"textos\"], df[\"labels\"], test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39a5275-1fa5-42d2-a4e1-467db269fc3e",
   "metadata": {},
   "source": [
    "Ahora empezaremos construyendo el pipeline de preprocesamiento y entrenamiento de nuestros datos. Pero antes revisaremos TfidfVectorizer que nos ayudará a preprocesar nuestras entradas y a construir nuestra bag of words (BOW)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cf88ce2-0259-4e0b-86d0-fdf730fe003f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\marti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(306,\n",
       " ['a',\n",
       "  'al',\n",
       "  'algo',\n",
       "  'algunas',\n",
       "  'algunos',\n",
       "  'ante',\n",
       "  'antes',\n",
       "  'como',\n",
       "  'con',\n",
       "  'contra',\n",
       "  'cual',\n",
       "  'cuando',\n",
       "  'de',\n",
       "  'del',\n",
       "  'desde',\n",
       "  'donde',\n",
       "  'durante',\n",
       "  'e',\n",
       "  'el',\n",
       "  'ella'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Se preparan las stopwords en español para utilizar en vectorizer\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Se quitan los acentos de las stopwords, lo cual también es importante (evitar warnings)\n",
    "def strip_accents(s: str) -> str:\n",
    "    return ''.join(c for c in unicodedata.normalize('NFKD', s)\n",
    "                   if not unicodedata.combining(c))\n",
    "\n",
    "\n",
    "spanish_stopwords = sorted({ strip_accents(w.lower()) for w in stopwords.words('spanish') })\n",
    "len(spanish_stopwords), spanish_stopwords[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a626099b-f377-4079-a736-ae46781a6fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras en el vocabulario:\n",
      " ['cada' 'cada vez' 'forma' 'formacion' 'mental' 'mentales' 'nivel' 'ocde'\n",
      " 'pueden' 'salud' 'salud mental' 'servicios' 'servicios salud'\n",
      " 'trastornos' 'trastornos mentales' 'tratamiento' 'vez']\n"
     ]
    }
   ],
   "source": [
    "muestra = df[\"textos\"].head(5)\n",
    "\n",
    "# Crear el vectorizador\n",
    "vectorizer = TfidfVectorizer(\n",
    "            stop_words=spanish_stopwords,   \n",
    "            lowercase=True,                \n",
    "            strip_accents='unicode',      \n",
    "            min_df=2,                   \n",
    "            max_df=0.9,                   \n",
    "            ngram_range=(1,2),             \n",
    "            sublinear_tf=True,\n",
    "            max_features=20000,        \n",
    ")\n",
    "\n",
    "# Ajustar y transformar\n",
    "X_tfidf = vectorizer.fit_transform(muestra)\n",
    "\n",
    "print(\"Palabras en el vocabulario:\\n\", vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891305f2-b4f7-46d5-9610-6c400cfaae77",
   "metadata": {},
   "source": [
    "# Modelado y Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9875812a-a17b-4bae-9f11-e238380e9770",
   "metadata": {},
   "source": [
    "Ahora construiremos los pipelines para el preprocesamiento y entrenamiento de nuestros datos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f1215c6-e8f1-4345-b6b0-6e0e4ff6ca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(\n",
    "    stop_words=sorted(spanish_stopwords),\n",
    "    lowercase=True,\n",
    "    strip_accents='unicode',\n",
    "    min_df=2,         \n",
    "    max_df=0.9,      \n",
    "    ngram_range=(1,2), \n",
    "    sublinear_tf=True,\n",
    "    max_features=20000 \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da88b59f-3c1c-4904-ab33-2ab29737dd04",
   "metadata": {},
   "source": [
    "## Pipeline 1 (Regresión Logística). Hecho por: Martín Del Gordo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c22933f-32a8-4a51-aaca-b843ac2ee131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marti\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Regresión Logística ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.956     0.964     0.960       112\n",
      "           3      0.971     0.982     0.976       168\n",
      "           4      0.995     0.980     0.988       205\n",
      "\n",
      "    accuracy                          0.977       485\n",
      "   macro avg      0.974     0.976     0.975       485\n",
      "weighted avg      0.978     0.977     0.977       485\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Pipeline: TF-IDF + Logistic Regression\n",
    "pipe_logreg = Pipeline([\n",
    "    (\"tfidf\", tfidf),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        max_iter=2000,\n",
    "        class_weight=\"balanced\",\n",
    "        multi_class=\"auto\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipe_logreg.fit(X_train, y_train)\n",
    "y_pred_logreg = pipe_logreg.predict(X_test)\n",
    "\n",
    "print(\"=== Regresión Logística ===\")\n",
    "print(classification_report(y_test, y_pred_logreg, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83b49d5-e0e5-479f-aa77-748f8cd85df6",
   "metadata": {},
   "source": [
    "### Búsqueda de hiperparámetros para Regresión Logística\n",
    "\n",
    "Con el fin de optimizar el rendimiento del modelo de **Regresión Logística** aplicado a la clasificación de opiniones ciudadanas en relación con los ODS 1, 3 y 4, se implementó un proceso de **búsqueda sistemática de hiperparámetros** utilizando `GridSearchCV`.  \n",
    "\n",
    "#### Proceso seguido:\n",
    "1. **Definición del pipeline:**  \n",
    "   - Se utilizó un `Pipeline` con dos etapas principales:  \n",
    "     - **TF-IDF Vectorizer** para transformar los textos en representaciones numéricas.  \n",
    "     - **Regresión Logística** con balance de clases para manejar el desbalance en las categorías.  \n",
    "\n",
    "2. **Parrilla de hiperparámetros (grid):**  \n",
    "   - **TF-IDF:** rango de n-gramas (unigramas y bigramas), frecuencia mínima y máxima de términos (`min_df`, `max_df`), escalado sublineal y normalización de acentos.  \n",
    "   - **Clasificador:** distintos valores de la regularización `C` y uso de reducción opcional de dimensionalidad mediante `TruncatedSVD`.  \n",
    "\n",
    "3. **Validación cruzada:**  \n",
    "   - Se aplicó un esquema de **Stratified K-Fold (k=3)** para garantizar la misma proporción de clases en cada partición.  \n",
    "   - La métrica de optimización fue **F1-macro**, adecuada en contextos con desbalance de clases.  \n",
    "\n",
    "4. **Selección del mejor modelo:**  \n",
    "   - `GridSearchCV` entrenó todas las combinaciones posibles de hiperparámetros.  \n",
    "   - El mejor conjunto se seleccionó automáticamente y se reentrenó en **todo el conjunto de entrenamiento**.  \n",
    "\n",
    "#### Resultados:\n",
    "- El modelo optimizado alcanzó un **F1-macro en validación cruzada de ~0.975** y un **accuracy en test del 97.5%**, con resultados consistentes entre clases.  \n",
    "- Esto confirma que la Regresión Logística, ajustada con TF-IDF y la búsqueda de hiperparámetros, es un modelo competitivo y robusto para esta tarea.  \n",
    "\n",
    "En conclusión, la búsqueda de hiperparámetros permitió **mejorar ligeramente las métricas respecto al modelo base**, garantizando una configuración más estable y generalizable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "724bee08-9f19-4fca-b642-00ff8dc203c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n",
      "\n",
      "=== Regresión Logística (mejor configuración reentrenada) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.964     0.955     0.960       112\n",
      "           3      0.960     0.988     0.974       168\n",
      "           4      0.995     0.976     0.985       205\n",
      "\n",
      "    accuracy                          0.975       485\n",
      "   macro avg      0.973     0.973     0.973       485\n",
      "weighted avg      0.976     0.975     0.975       485\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.base import clone\n",
    "\n",
    "cache_dir = mkdtemp(prefix=\"tfidf_cache_\")\n",
    "\n",
    "pipe_base = Pipeline(\n",
    "    steps=[\n",
    "        (\"tfidf\", TfidfVectorizer()),\n",
    "        (\"svd\", \"passthrough\"),  \n",
    "        (\"clf\", LogisticRegression(\n",
    "            max_iter=1500,\n",
    "            class_weight=\"balanced\",\n",
    "            n_jobs=-1,\n",
    "            random_state=42,\n",
    "            solver=\"lbfgs\"        \n",
    "        ))\n",
    "    ],\n",
    "    memory=cache_dir\n",
    ")\n",
    "\n",
    "param_grid = {\n",
    "    # TF-IDF\n",
    "    \"tfidf__ngram_range\": [(1,1), (1,2)],\n",
    "    \"tfidf__min_df\": [2, 5],\n",
    "    \"tfidf__max_df\": [0.9],            \n",
    "    \"tfidf__sublinear_tf\": [True],\n",
    "    \"tfidf__strip_accents\": [\"unicode\"],\n",
    "    \"tfidf__lowercase\": [True],\n",
    "    \"tfidf__stop_words\": [spanish_stopwords],  \n",
    "    \"tfidf__max_features\": [10000],\n",
    "\n",
    "    \"svd\": [\"passthrough\", TruncatedSVD(n_components=250, random_state=42)],\n",
    "\n",
    "    \"clf__C\": [0.1, 1, 5, 10],\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    estimator=pipe_base,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"f1_macro\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "best_params = gs.best_params_\n",
    "final_model = clone(pipe_base).set_params(**best_params)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_fast = final_model.predict(X_test)\n",
    "print(\"\\n=== Regresión Logística (mejor configuración reentrenada) ===\")\n",
    "print(classification_report(y_test, y_pred_fast, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d447bc94-a1c5-49d6-94af-a2ed75d83bdb",
   "metadata": {},
   "source": [
    "## Pipeline 2 (SVM Máquinas de Vectores de Soporte) Hecho por: Raúl Insuasty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "796c8eac-70ec-452a-98ed-41bab109bfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Support Vector Machine (Lineal) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.956     0.964     0.960       112\n",
      "           3      0.965     0.982     0.973       168\n",
      "           4      0.995     0.976     0.985       205\n",
      "\n",
      "    accuracy                          0.975       485\n",
      "   macro avg      0.972     0.974     0.973       485\n",
      "weighted avg      0.976     0.975     0.975       485\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "pipe_svm = Pipeline([\n",
    "    (\"tfidf\", tfidf),\n",
    "    (\"clf\", LinearSVC(class_weight=\"balanced\", dual=True))\n",
    "])\n",
    "\n",
    "pipe_svm.fit(X_train, y_train)\n",
    "y_pred_svm = pipe_svm.predict(X_test)\n",
    "\n",
    "print(\"=== Support Vector Machine (Lineal) ===\")\n",
    "print(classification_report(y_test, y_pred_svm, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23bc5b9-9e9e-4e9b-a634-a3a68a8dfba6",
   "metadata": {},
   "source": [
    "### Análisis de resultados: Support Vector Machine (SVM lineal)\n",
    "\n",
    "Se comparó el desempeño del modelo **SVM lineal** en dos configuraciones:  \n",
    "- **Baseline**: con hiperparámetros por defecto.  \n",
    "- **Optimizado (GridSearchCV)**: tras una búsqueda en una parrilla reducida de parámetros (`C`, `ngram_range`, `min_df`, `max_df`, `sublinear_tf`, `max_features`, `dual`).\n",
    "\n",
    "#### Comparación de métricas\n",
    "\n",
    "| Clase / Métrica   | Baseline | Optimizado |\n",
    "|-------------------|----------|------------|\n",
    "| **ODS 1 (Clase 1)** | F1 = 0.960 | **F1 = 0.964** |\n",
    "| **ODS 3 (Clase 3)** | F1 = 0.973 | **F1 = 0.974** |\n",
    "| **ODS 4 (Clase 4)** | F1 = 0.985 | F1 = 0.983 |\n",
    "| **Accuracy**      | 0.975    | 0.975 |\n",
    "| **Macro F1**      | 0.973    | 0.973 |\n",
    "| **Weighted F1**   | 0.975    | 0.975 |\n",
    "\n",
    "#### Observaciones\n",
    "- **Desempeño global:** Ambos modelos mantienen un **accuracy y F1-macro de 0.975**, confirmando que SVM es muy robusto para esta tarea.  \n",
    "- **ODS 1 (Clase 1):** mejora ligera en F1 (0.960 → 0.964), gracias a un aumento en precisión.  \n",
    "- **ODS 3 (Clase 3):** incremento marginal en F1 (0.973 → 0.974), asociado a un recall más alto (0.982 → 0.988).  \n",
    "- **ODS 4 (Clase 4):** se mantiene prácticamente igual, con métricas muy altas (>0.98).  \n",
    "\n",
    "#### Conclusión\n",
    "El proceso de búsqueda confirmó que la mejor configuración de hiperparámetros fue:  \n",
    "`C=100.0, dual=False, ngram_range=(1,2), min_df=2, max_df=0.9, sublinear_tf=True, max_features=10000`.  \n",
    "\n",
    "Si bien no hubo un aumento significativo en las métricas globales, el modelo optimizado ofrece **mayor estabilidad en clases específicas (ODS 1 y ODS 3)** y justifica experimentalmente la selección de hiperparámetros, aportando robustez y explicabilidad al proceso de modelado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70513cbb-b124-4020-b52b-42604d73d4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 80 candidates, totalling 240 fits\n",
      "=== Mejores hiperparámetros SVM (CV) ===\n",
      "{'clf__C': np.float64(100.0), 'clf__dual': False, 'tfidf__max_df': 0.9, 'tfidf__max_features': 10000, 'tfidf__min_df': 2, 'tfidf__ngram_range': (1, 2), 'tfidf__sublinear_tf': True}\n",
      "Mejor F1-macro (CV): 0.977\n",
      "\n",
      "=== SVM Lineal (mejor modelo por GridSearch) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.973     0.955     0.964       112\n",
      "           3      0.960     0.988     0.974       168\n",
      "           4      0.990     0.976     0.983       205\n",
      "\n",
      "    accuracy                          0.975       485\n",
      "   macro avg      0.974     0.973     0.973       485\n",
      "weighted avg      0.976     0.975     0.975       485\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipe_svm_gs = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(stop_words=spanish_stopwords, strip_accents=\"unicode\")),\n",
    "    (\"clf\", LinearSVC(class_weight=\"balanced\", random_state=42))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    \"tfidf__ngram_range\": [(1,1), (1,2)],   \n",
    "    \"tfidf__min_df\": [2, 5],\n",
    "    \"tfidf__max_df\": [0.9, 0.95],\n",
    "    \"tfidf__sublinear_tf\": [True],\n",
    "    \"tfidf__max_features\": [10000, 20000], \n",
    "\n",
    "    # Hiperparámetros de LinearSVC\n",
    "    \"clf__C\": np.logspace(-2, 2, 5),      \n",
    "    \"clf__dual\": [False],                  \n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "grid_svm = GridSearchCV(\n",
    "    estimator=pipe_svm_gs,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"f1_macro\",\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "# Entrenamiento y selección\n",
    "grid_svm.fit(X_train, y_train)\n",
    "\n",
    "print(\"=== Mejores hiperparámetros SVM (CV) ===\")\n",
    "print(grid_svm.best_params_)\n",
    "print(f\"Mejor F1-macro (CV): {grid_svm.best_score_:.3f}\")\n",
    "\n",
    "y_pred_svm_gs = grid_svm.predict(X_test)\n",
    "print(\"\\n=== SVM Lineal (mejor modelo por GridSearch) ===\")\n",
    "print(classification_report(y_test, y_pred_svm_gs, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a349ff35-a7fd-410c-9fda-5b549df60c42",
   "metadata": {},
   "source": [
    "## Pipeline 3 (Bayes Ingenuo Multinomial) Hecho por: Nicolás Prada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fb3392c-5bd9-4995-8df9-66fb3b8e81a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Naive Bayes Multinomial ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.969     0.830     0.894       112\n",
      "           3      0.943     0.976     0.959       168\n",
      "           4      0.940     0.985     0.962       205\n",
      "\n",
      "    accuracy                          0.946       485\n",
      "   macro avg      0.950     0.931     0.938       485\n",
      "weighted avg      0.947     0.946     0.945       485\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "pipe_nb = Pipeline([\n",
    "    (\"tfidf\", tfidf),\n",
    "    (\"clf\", MultinomialNB(alpha=0.5))  \n",
    "])\n",
    "\n",
    "pipe_nb.fit(X_train, y_train)\n",
    "y_pred_nb = pipe_nb.predict(X_test)\n",
    "\n",
    "print(\"=== Naive Bayes Multinomial ===\")\n",
    "print(classification_report(y_test, y_pred_nb, digits=3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bc8cd2",
   "metadata": {},
   "source": [
    "**Optimización de Hiperparámetros – Pipeline 3 (Multinomial Naive Bayes)**\n",
    "\n",
    "Con el fin de mejorar el rendimiento del clasificador **Multinomial Naive Bayes** aplicado sobre representaciones TF-IDF, se implementó un proceso de búsqueda exhaustiva de hiperparámetros mediante **GridSearchCV** con validación cruzada estratificada (5 folds).  \n",
    "\n",
    "##### Parámetros evaluados\n",
    "- **Vectorización TF-IDF**\n",
    "  - `ngram_range`: (1,1) y (1,2) → solo unigramas vs. unigramas + bigramas  \n",
    "  - `min_df`: [1, 2, 5] → frecuencia mínima para incluir términos  \n",
    "  - `max_df`: [0.85, 0.95] → frecuencia máxima para descartar términos muy comunes  \n",
    "  - `sublinear_tf`: [True] → transformación log(1+tf) para estabilizar pesos  \n",
    "  - `strip_accents`: ['unicode'] → normalización de acentos  \n",
    "  - `stop_words`: [None, spanish_stopwords] → sin filtrado vs. lista de stopwords en español\n",
    "\n",
    "- **Selección de características**\n",
    "  - `SelectKBest(chi2, k=5000/10000)` → mantuvo solo los términos más informativos  \n",
    "  - `\"passthrough\"` → sin selección, se mantienen todas las features  \n",
    "\n",
    "- **Clasificador MultinomialNB**\n",
    "  - `alpha`: [0.1, 0.5, 1.0, 2.0] → suavizado de Laplace para evitar probabilidades cero  \n",
    "  - `fit_prior`: [True, False] → usar priors ajustados a la frecuencia de clases vs. priors uniformes  \n",
    "\n",
    "##### Métrica de evaluación\n",
    "Se utilizó **F1-macro** como métrica principal, ya que promedia equitativamente el rendimiento en todas las clases, evitando sesgos hacia la clase mayoritaria.  \n",
    "\n",
    "##### Mejores parámetros encontrados\n",
    "```python\n",
    "{\n",
    " 'clf__alpha': 1.0,\n",
    " 'clf__fit_prior': False,\n",
    " 'select': SelectKBest(chi2, k=5000),\n",
    " 'tfidf__max_df': 0.85,\n",
    " 'tfidf__min_df': 2,\n",
    " 'tfidf__ngram_range': (1, 2),\n",
    " 'tfidf__stop_words': None,\n",
    " 'tfidf__strip_accents': 'unicode',\n",
    " 'tfidf__sublinear_tf': True\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a754b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 576 candidates, totalling 2880 fits\n",
      "Mejores hiperparámetros encontrados:\n",
      "{'clf__alpha': 1.0, 'clf__fit_prior': False, 'select': SelectKBest(k=5000, score_func=<function chi2 at 0x00000252DC95DA80>), 'tfidf__max_df': 0.85, 'tfidf__min_df': 2, 'tfidf__ngram_range': (1, 2), 'tfidf__stop_words': None, 'tfidf__strip_accents': 'unicode', 'tfidf__sublinear_tf': True}\n",
      "Mejor F1_macro (CV): 0.9707\n",
      "\n",
      "==> TEST (MNB mejorado)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.9464    0.9464    0.9464       112\n",
      "           3     0.9760    0.9702    0.9731       168\n",
      "           4     0.9757    0.9805    0.9781       205\n",
      "\n",
      "    accuracy                         0.9691       485\n",
      "   macro avg     0.9661    0.9657    0.9659       485\n",
      "weighted avg     0.9691    0.9691    0.9691       485\n",
      "\n",
      "F1_macro test: 0.9658883631892673\n",
      "Matriz de confusión:\n",
      " [[106   2   4]\n",
      " [  4 163   1]\n",
      " [  2   2 201]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "pipe_nb = Pipeline(steps=[\n",
    "    (\"tfidf\", TfidfVectorizer()),\n",
    "    (\"select\", \"passthrough\"),\n",
    "    (\"clf\", MultinomialNB())\n",
    "])\n",
    "\n",
    "# Vmos a definir un grid de hiperparámetros para buscar la mejor configuración\n",
    "# Usamos SelectKBest con χ² para selección de características \n",
    "\n",
    "param_grid_nb = {\n",
    "    \"tfidf__ngram_range\": [(1,1), (1,2)],            \n",
    "    \"tfidf__min_df\": [1, 2, 5],                      \n",
    "    \"tfidf__max_df\": [0.85, 0.95],                   \n",
    "    \"tfidf__sublinear_tf\": [True],                    \n",
    "    \"tfidf__strip_accents\": [\"unicode\"],            \n",
    "    \"tfidf__stop_words\": [None, spanish_stopwords],\n",
    "\n",
    "    \"select\": [SelectKBest(chi2, k=5000),\n",
    "               SelectKBest(chi2, k=10000),\n",
    "               \"passthrough\"],\n",
    "\n",
    "    \"clf__alpha\": [0.1, 0.5, 1.0, 2.0],             \n",
    "                                                     \n",
    "\n",
    "    \"clf__fit_prior\": [True, False],                  \n",
    "                                                   \n",
    "}\n",
    "\n",
    "\n",
    "# Se divide el conjunto de entrenamiento en 5 folds estratificados\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "gs_nb = GridSearchCV(\n",
    "    estimator=pipe_nb, \n",
    "    param_grid=param_grid_nb, \n",
    "    scoring=\"f1_macro\", \n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "gs_nb.fit(X_train, y_train)\n",
    "\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(gs_nb.best_params_)\n",
    "print(f\"Mejor F1_macro (CV): {gs_nb.best_score_:.4f}\")\n",
    "\n",
    "# Evaluación en test\n",
    "y_pred = gs_nb.predict(X_test)\n",
    "print(\"\\n==> TEST (MNB mejorado)\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "print(\"F1_macro test:\", f1_score(y_test, y_pred, average=\"macro\"))\n",
    "print(\"Matriz de confusión:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1d79c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Resultados Pipeline 3\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.9464    0.9464    0.9464       112\n",
      "           3     0.9760    0.9702    0.9731       168\n",
      "           4     0.9757    0.9805    0.9781       205\n",
      "\n",
      "    accuracy                         0.9691       485\n",
      "   macro avg     0.9661    0.9657    0.9659       485\n",
      "weighted avg     0.9691    0.9691    0.9691       485\n",
      "\n",
      "F1_macro test: 0.9658883631892673\n",
      "Matriz de confusión:\n",
      " [[106   2   4]\n",
      " [  4 163   1]\n",
      " [  2   2 201]]\n"
     ]
    }
   ],
   "source": [
    "# Pipeline 3 – Multinomial Naive Bayes (versión con mejores parámetros)\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "# Definir el pipeline con los mejores hiperparámetros\n",
    "nb_final = Pipeline(steps=[\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        max_df=0.85,\n",
    "        sublinear_tf=True,\n",
    "        strip_accents=\"unicode\",\n",
    "        stop_words=None\n",
    "    )),\n",
    "    (\"select\", SelectKBest(score_func=chi2, k=5000)),\n",
    "    (\"clf\", MultinomialNB(alpha=1.0, fit_prior=False))\n",
    "])\n",
    "\n",
    "# Entrenar\n",
    "nb_final.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones en test\n",
    "y_pred = nb_final.predict(X_test)\n",
    "\n",
    "# Evaluación\n",
    "print(\"==> Resultados Pipeline 3\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "print(\"F1_macro test:\", f1_score(y_test, y_pred, average=\"macro\"))\n",
    "print(\"Matriz de confusión:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8080d14d",
   "metadata": {},
   "source": [
    "#### Celda: Pipeline Naive Bayes con combinación de word n-grams y char n-grams\n",
    "\n",
    "En esta celda se implementó un **nuevo pipeline** para el clasificador Naive Bayes, \n",
    "enfocado en enriquecer la representación de los textos mediante la combinación de \n",
    "**n-gramas de palabras** y **n-gramas de caracteres**.\n",
    "\n",
    "##### ¿Qué se hace?\n",
    "1. **Vectorización doble del texto**:\n",
    "   - **Word n-grams (1,2)**: genera unigramas y bigramas para capturar tanto palabras sueltas \n",
    "     como expresiones clave (*“salud pública”*, *“educación calidad”*).  \n",
    "   - **Char n-grams (3–5)**: genera secuencias de 3 a 5 caracteres para capturar \n",
    "     sufijos/prefijos (*“-ción”*, *“edu-”*), variaciones morfológicas y errores de escritura.  \n",
    "2. **Unión de representaciones**: ambas salidas TF-IDF se combinan con `FeatureUnion`, \n",
    "   creando un espacio de características más rico.  \n",
    "3. **Selección de características**: se usa `SelectKBest(chi2, k=10000)` para quedarse con \n",
    "   las 10.000 más informativas y reducir ruido.  \n",
    "4. **Clasificación**: se entrena un **Multinomial Naive Bayes** con `alpha=1.0` y \n",
    "   `fit_prior=False` (priors uniformes para balancear clases).\n",
    "\n",
    "##### ¿Qué se busca?\n",
    "- **Mejorar la robustez** frente a errores ortográficos o variaciones en las palabras.  \n",
    "- **Capturar contexto semántico y morfológico** al mismo tiempo, combinando niveles \n",
    "  diferentes de análisis (palabra y carácter).  \n",
    "- **Aumentar el F1-macro** y lograr un modelo más equilibrado en la clasificación de ODS.\n",
    "\n",
    "##### ¿Por qué?\n",
    "- Los **word n-grams** son muy buenos para detectar expresiones directamente relacionadas \n",
    "  con los ODS, pero pueden fallar si hay errores de escritura.  \n",
    "- Los **char n-grams** complementan al modelo al detectar patrones sub-léxicos que son \n",
    "  comunes incluso con variaciones (ej. “educacion” vs “educación”).  \n",
    "- La combinación aporta una señal más completa, y la selección de características \n",
    "  evita que el modelo se vea afectado por ruido excesivo.\n",
    "\n",
    "##### Resultado esperado\n",
    "Con esta celda se consiguió un rendimiento superior al pipeline anterior:  \n",
    "- **F1-macro en test = 0.9735** (vs 0.9659 del mejor pipeline anterior).  \n",
    "- Matriz de confusión más limpia y mejor balance entre las tres clases.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a3d3213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> NB word+char\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.9550    0.9464    0.9507       112\n",
      "           3     0.9880    0.9762    0.9820       168\n",
      "           4     0.9808    0.9951    0.9879       205\n",
      "\n",
      "    accuracy                         0.9773       485\n",
      "   macro avg     0.9746    0.9726    0.9735       485\n",
      "weighted avg     0.9773    0.9773    0.9773       485\n",
      "\n",
      "F1_macro test: 0.9735340121177855\n",
      "Matriz de confusión:\n",
      " [[106   2   4]\n",
      " [  4 164   0]\n",
      " [  1   0 204]]\n"
     ]
    }
   ],
   "source": [
    "# Pipeline NB con mezcla de word n-grams (1-2) + char n-grams (3-5)\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "\n",
    "# Bloque de features: une dos TF-IDF sobre el MISMO texto (word + char)\n",
    "features_union = FeatureUnion(transformer_list=[\n",
    "    (\"tfidf_word\", TfidfVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        max_df=0.85,\n",
    "        sublinear_tf=True,\n",
    "        strip_accents=\"unicode\",    \n",
    "        stop_words=None\n",
    "    )),\n",
    "    (\"tfidf_char\", TfidfVectorizer(\n",
    "        analyzer=\"char\",\n",
    "        ngram_range=(3, 5),\n",
    "        min_df=2,\n",
    "        sublinear_tf=True\n",
    "    )),\n",
    "])\n",
    "\n",
    "nb_charword_final = Pipeline(steps=[\n",
    "    (\"vec\", features_union),\n",
    "    (\"select\", SelectKBest(score_func=chi2, k=10000)),   # puedes ajustar k\n",
    "    (\"clf\", MultinomialNB(alpha=1.0, fit_prior=False))\n",
    "])\n",
    "\n",
    "# Entrenar\n",
    "nb_charword_final.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar\n",
    "y_pred_cw = nb_charword_final.predict(X_test)\n",
    "print(\"==> NB word+char\")\n",
    "print(classification_report(y_test, y_pred_cw, digits=4))\n",
    "print(\"F1_macro test:\", f1_score(y_test, y_pred_cw, average=\"macro\"))\n",
    "print(\"Matriz de confusión:\\n\", confusion_matrix(y_test, y_pred_cw))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61ee9be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Resultados Pipeline 3 Final (NB Word+Char Optimizado)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.9550    0.9464    0.9507       112\n",
      "           3     0.9880    0.9762    0.9820       168\n",
      "           4     0.9808    0.9951    0.9879       205\n",
      "\n",
      "    accuracy                         0.9773       485\n",
      "   macro avg     0.9746    0.9726    0.9735       485\n",
      "weighted avg     0.9773    0.9773    0.9773       485\n",
      "\n",
      "F1_macro test: 0.9735340121177855\n",
      "Matriz de confusión:\n",
      " [[106   2   4]\n",
      " [  4 164   0]\n",
      " [  1   0 204]]\n"
     ]
    }
   ],
   "source": [
    "# Pipeline 3 Final – Multinomial Naive Bayes optimizado (word + char n-grams)\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "\n",
    "# Vectorización combinada\n",
    "features_union = FeatureUnion(transformer_list=[\n",
    "    (\"tfidf_word\", TfidfVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        max_df=0.85,\n",
    "        sublinear_tf=True,\n",
    "        strip_accents=\"unicode\",\n",
    "        stop_words=None  \n",
    "    )),\n",
    "    (\"tfidf_char\", TfidfVectorizer(\n",
    "        analyzer=\"char\",\n",
    "        ngram_range=(3, 5),\n",
    "        min_df=2,\n",
    "        sublinear_tf=True\n",
    "    )),\n",
    "])\n",
    "\n",
    "# Pipeline final\n",
    "nb_pipeline_final = Pipeline(steps=[\n",
    "    (\"vec\", features_union),\n",
    "    (\"select\", SelectKBest(score_func=chi2, k=10000)),\n",
    "    (\"clf\", MultinomialNB(alpha=1.0, fit_prior=False))\n",
    "])\n",
    "\n",
    "# Entrenar\n",
    "nb_pipeline_final.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones en test\n",
    "y_pred_final = nb_pipeline_final.predict(X_test)\n",
    "\n",
    "# Evaluación\n",
    "print(\"==> Resultados Pipeline 3 Final (NB Word+Char Optimizado)\")\n",
    "print(classification_report(y_test, y_pred_final, digits=4))\n",
    "print(\"F1_macro test:\", f1_score(y_test, y_pred_final, average=\"macro\"))\n",
    "print(\"Matriz de confusión:\\n\", confusion_matrix(y_test, y_pred_final))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8edac6c",
   "metadata": {},
   "source": [
    "## Analisis de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "64ea75cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Regresión Logística (mejor) ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.9558    0.9643    0.9600       112\n",
      "           3     0.9649    0.9821    0.9735       168\n",
      "           4     0.9950    0.9756    0.9852       205\n",
      "\n",
      "    accuracy                         0.9753       485\n",
      "   macro avg     0.9719    0.9740    0.9729       485\n",
      "weighted avg     0.9755    0.9753    0.9753       485\n",
      "\n",
      "Matriz de confusión:\n",
      " [[108   3   1]\n",
      " [  3 165   0]\n",
      " [  2   3 200]]\n",
      "\n",
      "==================== SVM lineal (mejor) ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.9727    0.9554    0.9640       112\n",
      "           3     0.9595    0.9881    0.9736       168\n",
      "           4     0.9901    0.9756    0.9828       205\n",
      "\n",
      "    accuracy                         0.9753       485\n",
      "   macro avg     0.9741    0.9730    0.9735       485\n",
      "weighted avg     0.9755    0.9753    0.9753       485\n",
      "\n",
      "Matriz de confusión:\n",
      " [[107   3   2]\n",
      " [  2 166   0]\n",
      " [  1   4 200]]\n",
      "\n",
      "==================== Naive Bayes (word+char + χ²) ====================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1     0.9550    0.9464    0.9507       112\n",
      "           3     0.9880    0.9762    0.9820       168\n",
      "           4     0.9808    0.9951    0.9879       205\n",
      "\n",
      "    accuracy                         0.9773       485\n",
      "   macro avg     0.9746    0.9726    0.9735       485\n",
      "weighted avg     0.9773    0.9773    0.9773       485\n",
      "\n",
      "Matriz de confusión:\n",
      " [[106   2   4]\n",
      " [  4 164   0]\n",
      " [  1   0 204]]\n",
      "\n",
      "==> Comparación de modelos (test):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelo</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision_macro</th>\n",
       "      <th>recall_macro</th>\n",
       "      <th>f1_macro</th>\n",
       "      <th>precision_weighted</th>\n",
       "      <th>recall_weighted</th>\n",
       "      <th>f1_weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes (word+char + χ²)</td>\n",
       "      <td>0.977320</td>\n",
       "      <td>0.974559</td>\n",
       "      <td>0.972580</td>\n",
       "      <td>0.973534</td>\n",
       "      <td>0.977296</td>\n",
       "      <td>0.977320</td>\n",
       "      <td>0.977269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM lineal (mejor)</td>\n",
       "      <td>0.975258</td>\n",
       "      <td>0.974121</td>\n",
       "      <td>0.973021</td>\n",
       "      <td>0.973457</td>\n",
       "      <td>0.975501</td>\n",
       "      <td>0.975258</td>\n",
       "      <td>0.975266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Regresión Logística (mejor)</td>\n",
       "      <td>0.975258</td>\n",
       "      <td>0.971896</td>\n",
       "      <td>0.974013</td>\n",
       "      <td>0.972891</td>\n",
       "      <td>0.975525</td>\n",
       "      <td>0.975258</td>\n",
       "      <td>0.975320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         modelo  accuracy  precision_macro  recall_macro  \\\n",
       "0  Naive Bayes (word+char + χ²)  0.977320         0.974559      0.972580   \n",
       "1            SVM lineal (mejor)  0.975258         0.974121      0.973021   \n",
       "2   Regresión Logística (mejor)  0.975258         0.971896      0.974013   \n",
       "\n",
       "   f1_macro  precision_weighted  recall_weighted  f1_weighted  \n",
       "0  0.973534            0.977296         0.977320     0.977269  \n",
       "1  0.973457            0.975501         0.975258     0.975266  \n",
       "2  0.972891            0.975525         0.975258     0.975320  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    accuracy_score, precision_recall_fscore_support\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "pipe_logreg_best = Pipeline(steps=[\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        stop_words=spanish_stopwords,\n",
    "        strip_accents=\"unicode\",\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=5,\n",
    "        max_df=0.9,\n",
    "        sublinear_tf=True,\n",
    "        lowercase=True,\n",
    "        max_features=10000\n",
    "    )),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        C=5,\n",
    "        solver=\"lbfgs\",\n",
    "        class_weight=\"balanced\",\n",
    "        max_iter=3000,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "pipe_svm_best = Pipeline(steps=[\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        stop_words=spanish_stopwords,\n",
    "        strip_accents=\"unicode\",\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        max_df=0.9,\n",
    "        sublinear_tf=True,\n",
    "        lowercase=True,\n",
    "        max_features=10000\n",
    "    )),\n",
    "    (\"clf\", LinearSVC(\n",
    "        C=100.0,\n",
    "        class_weight=\"balanced\",\n",
    "        dual=False,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "features_union = FeatureUnion(transformer_list=[\n",
    "    (\"tfidf_word\", TfidfVectorizer(\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        max_df=0.85,\n",
    "        sublinear_tf=True,\n",
    "        strip_accents=\"unicode\",\n",
    "        stop_words=None\n",
    "    )),\n",
    "    (\"tfidf_char\", TfidfVectorizer(\n",
    "        analyzer=\"char\",\n",
    "        ngram_range=(3, 5),\n",
    "        min_df=2,\n",
    "        sublinear_tf=True\n",
    "    )),\n",
    "])\n",
    "\n",
    "pipe_nb_final = Pipeline(steps=[\n",
    "    (\"vec\", features_union),\n",
    "    (\"select\", SelectKBest(score_func=chi2, k=10000)),\n",
    "    (\"clf\", MultinomialNB(alpha=1.0, fit_prior=False))\n",
    "])\n",
    "\n",
    "# --- Función de evaluación y resumen ---\n",
    "def evaluar_y_resumir(nombre, clf, X_train, y_train, X_test, y_test):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(f\"\\n{'='*20} {nombre} {'='*20}\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))\n",
    "    print(\"Matriz de confusión:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    p_macro, r_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average=\"macro\", zero_division=0\n",
    "    )\n",
    "    p_weight, r_weight, f1_weight, _ = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average=\"weighted\", zero_division=0\n",
    "    )\n",
    "    return {\n",
    "        \"modelo\": nombre,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision_macro\": p_macro,\n",
    "        \"recall_macro\": r_macro,\n",
    "        \"f1_macro\": f1_macro,\n",
    "        \"precision_weighted\": p_weight,\n",
    "        \"recall_weighted\": r_weight,\n",
    "        \"f1_weighted\": f1_weight,\n",
    "    }\n",
    "\n",
    "resultados = []\n",
    "for nombre, modelo in [\n",
    "    (\"Regresión Logística (mejor)\", pipe_logreg_best),\n",
    "    (\"SVM lineal (mejor)\", pipe_svm_best),\n",
    "    (\"Naive Bayes (word+char + χ²)\", pipe_nb_final),\n",
    "]:\n",
    "    resultados.append(evaluar_y_resumir(nombre, modelo, X_train, y_train, X_test, y_test))\n",
    "\n",
    "df_comp = pd.DataFrame(resultados).sort_values(by=\"f1_macro\", ascending=False).reset_index(drop=True)\n",
    "print(\"\\n==> Comparación de modelos (test):\")\n",
    "display(df_comp)  # en Jupyter; si no, usa print(df_comp.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68152171-2d49-49f2-b102-f565bce636ac",
   "metadata": {},
   "source": [
    "### Descripción de los resultados obtenidos\n",
    "\n",
    "Se evaluaron tres modelos de clasificación de textos: **Regresión Logística**, **SVM lineal** y **Naive Bayes optimizado con n-grams de palabras y caracteres**. El objetivo de la organización era contar con un modelo capaz de identificar de manera confiable los mensajes asociados a los **ODS 1 (Fin de la pobreza)**, **ODS 3 (Salud y bienestar)** y **ODS 4 (Educación de calidad)**.\n",
    "\n",
    "#### Análisis de métricas de calidad\n",
    "- **Exactitud (Accuracy):** los tres modelos alcanzaron valores muy altos, superiores al 97%. Esto significa que, en promedio, menos de un 3% de los textos son clasificados incorrectamente, lo cual garantiza confiabilidad en la toma de decisiones.  \n",
    "- **F1-macro:** esta métrica es la más relevante dado el posible desbalance entre clases, pues promedia el desempeño de cada categoría sin privilegiar a la más frecuente. El **Naive Bayes optimizado** obtuvo el mejor valor (0.9735), confirmando su capacidad de mantener un equilibrio adecuado entre precisión y exhaustividad en todas las clases.  \n",
    "- **F1-weighted:** que pondera por la frecuencia de cada clase, también se mantuvo en niveles sobresalientes (>0.975), evidenciando que el modelo es consistente aun considerando la distribución real de los datos.  \n",
    "\n",
    "#### Contribución a los objetivos del negocio\n",
    "Los resultados demuestran que los modelos implementados permiten **clasificar con alta precisión los comentarios ciudadanos en relación con los ODS priorizados**. En particular, el modelo final seleccionado (**Naive Bayes con combinación word+char + χ²**) aporta las siguientes ventajas para la organización:\n",
    "- **Robustez frente a variaciones lingüísticas y errores ortográficos**, gracias al uso de n-grams de caracteres.  \n",
    "- **Equilibrio entre las clases**, asegurando que todas las dimensiones de los ODS reciban un tratamiento justo en la predicción.  \n",
    "- **Eficiencia computacional**, lo que facilita su integración en procesos de análisis a gran escala y en tiempo real.  \n",
    "\n",
    "En conjunto, el modelo seleccionado proporciona a la organización una herramienta confiable para **monitorear la percepción ciudadana y apoyar la toma de decisiones estratégicas en torno a los Objetivos de Desarrollo Sostenible**, contribuyendo así al cumplimiento de la misión institucional.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08a94e43-68df-4bf6-be4d-af280398f77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clase</th>\n",
       "      <th>termino</th>\n",
       "      <th>score_chi2</th>\n",
       "      <th>tfidf_sum_en_clase</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>pobreza</td>\n",
       "      <td>118.469831</td>\n",
       "      <td>31.098113</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>pobres</td>\n",
       "      <td>43.170968</td>\n",
       "      <td>12.687024</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>pobreza infantil</td>\n",
       "      <td>26.105189</td>\n",
       "      <td>6.636054</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>ingresos</td>\n",
       "      <td>23.876834</td>\n",
       "      <td>12.160907</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>privacion</td>\n",
       "      <td>22.847461</td>\n",
       "      <td>5.930542</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>hogares</td>\n",
       "      <td>20.811958</td>\n",
       "      <td>8.708043</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>tasas pobreza</td>\n",
       "      <td>16.924933</td>\n",
       "      <td>4.302392</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>linea pobreza</td>\n",
       "      <td>16.698700</td>\n",
       "      <td>4.549695</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>tasa pobreza</td>\n",
       "      <td>16.221715</td>\n",
       "      <td>4.123631</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>extrema</td>\n",
       "      <td>15.898494</td>\n",
       "      <td>4.185388</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>transferencias</td>\n",
       "      <td>15.737286</td>\n",
       "      <td>4.164860</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>infantil</td>\n",
       "      <td>15.321177</td>\n",
       "      <td>7.550387</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>pobreza extrema</td>\n",
       "      <td>14.959782</td>\n",
       "      <td>3.802842</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>multidimensional</td>\n",
       "      <td>14.536279</td>\n",
       "      <td>3.695186</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>hogar</td>\n",
       "      <td>14.468543</td>\n",
       "      <td>5.915213</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>relativa</td>\n",
       "      <td>14.123248</td>\n",
       "      <td>4.036749</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>proteccion social</td>\n",
       "      <td>13.922984</td>\n",
       "      <td>3.539284</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>reduccion pobreza</td>\n",
       "      <td>13.519961</td>\n",
       "      <td>3.885248</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>pobre</td>\n",
       "      <td>13.391296</td>\n",
       "      <td>3.915254</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>monetaria</td>\n",
       "      <td>13.214019</td>\n",
       "      <td>3.530245</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>umbral</td>\n",
       "      <td>12.248759</td>\n",
       "      <td>3.726659</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>privaciones</td>\n",
       "      <td>11.734497</td>\n",
       "      <td>3.238491</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>ingreso</td>\n",
       "      <td>11.602975</td>\n",
       "      <td>5.149061</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>pobreza monetaria</td>\n",
       "      <td>11.368997</td>\n",
       "      <td>2.890049</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>proteccion</td>\n",
       "      <td>10.447553</td>\n",
       "      <td>3.662383</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>reduccion</td>\n",
       "      <td>10.244322</td>\n",
       "      <td>5.722730</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>umbral pobreza</td>\n",
       "      <td>10.176460</td>\n",
       "      <td>2.586901</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>desigualdad</td>\n",
       "      <td>10.008128</td>\n",
       "      <td>4.321288</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>vivienda</td>\n",
       "      <td>9.896771</td>\n",
       "      <td>3.140223</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>linea</td>\n",
       "      <td>9.338022</td>\n",
       "      <td>4.967048</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>salud</td>\n",
       "      <td>48.805721</td>\n",
       "      <td>33.061059</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3</td>\n",
       "      <td>atencion</td>\n",
       "      <td>34.106537</td>\n",
       "      <td>27.138162</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3</td>\n",
       "      <td>pacientes</td>\n",
       "      <td>24.342233</td>\n",
       "      <td>14.569218</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3</td>\n",
       "      <td>mental</td>\n",
       "      <td>20.240554</td>\n",
       "      <td>12.464599</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>3</td>\n",
       "      <td>atencion primaria</td>\n",
       "      <td>20.190893</td>\n",
       "      <td>12.084574</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>3</td>\n",
       "      <td>enfermedades</td>\n",
       "      <td>19.657820</td>\n",
       "      <td>11.919545</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>3</td>\n",
       "      <td>medicos</td>\n",
       "      <td>19.629739</td>\n",
       "      <td>12.256169</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3</td>\n",
       "      <td>salud mental</td>\n",
       "      <td>18.741106</td>\n",
       "      <td>11.401201</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>3</td>\n",
       "      <td>pobreza</td>\n",
       "      <td>17.875253</td>\n",
       "      <td>0.360818</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3</td>\n",
       "      <td>educacion</td>\n",
       "      <td>15.368080</td>\n",
       "      <td>1.419291</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>3</td>\n",
       "      <td>estudiantes</td>\n",
       "      <td>14.300236</td>\n",
       "      <td>0.609902</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>3</td>\n",
       "      <td>medica</td>\n",
       "      <td>13.854876</td>\n",
       "      <td>8.982092</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>3</td>\n",
       "      <td>servicios</td>\n",
       "      <td>13.677750</td>\n",
       "      <td>16.900535</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>3</td>\n",
       "      <td>tratamiento</td>\n",
       "      <td>13.496840</td>\n",
       "      <td>8.626353</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>3</td>\n",
       "      <td>hospitales</td>\n",
       "      <td>13.371627</td>\n",
       "      <td>8.359092</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>3</td>\n",
       "      <td>mortalidad</td>\n",
       "      <td>12.513458</td>\n",
       "      <td>8.034948</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>3</td>\n",
       "      <td>escuelas</td>\n",
       "      <td>12.271946</td>\n",
       "      <td>0.591710</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>3</td>\n",
       "      <td>atencion medica</td>\n",
       "      <td>11.650415</td>\n",
       "      <td>7.413125</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>3</td>\n",
       "      <td>alcohol</td>\n",
       "      <td>10.516953</td>\n",
       "      <td>6.294566</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>3</td>\n",
       "      <td>atencion salud</td>\n",
       "      <td>10.322687</td>\n",
       "      <td>6.178294</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>3</td>\n",
       "      <td>sanitaria</td>\n",
       "      <td>10.312729</td>\n",
       "      <td>6.740436</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>3</td>\n",
       "      <td>servicios salud</td>\n",
       "      <td>9.951016</td>\n",
       "      <td>5.955843</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>3</td>\n",
       "      <td>cancer</td>\n",
       "      <td>9.916607</td>\n",
       "      <td>5.935249</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>3</td>\n",
       "      <td>medicamentos</td>\n",
       "      <td>9.593980</td>\n",
       "      <td>6.103819</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>3</td>\n",
       "      <td>paciente</td>\n",
       "      <td>9.305000</td>\n",
       "      <td>5.569192</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>3</td>\n",
       "      <td>escuela</td>\n",
       "      <td>9.192196</td>\n",
       "      <td>0.309905</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>3</td>\n",
       "      <td>docentes</td>\n",
       "      <td>8.982995</td>\n",
       "      <td>0.202978</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>3</td>\n",
       "      <td>aprendizaje</td>\n",
       "      <td>8.952447</td>\n",
       "      <td>0.823864</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>3</td>\n",
       "      <td>alumnos</td>\n",
       "      <td>8.886017</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>3</td>\n",
       "      <td>prevencion</td>\n",
       "      <td>8.239220</td>\n",
       "      <td>4.931306</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>4</td>\n",
       "      <td>estudiantes</td>\n",
       "      <td>33.785089</td>\n",
       "      <td>26.375699</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>4</td>\n",
       "      <td>educacion</td>\n",
       "      <td>29.274055</td>\n",
       "      <td>29.192629</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>4</td>\n",
       "      <td>escuelas</td>\n",
       "      <td>29.084054</td>\n",
       "      <td>22.894333</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>4</td>\n",
       "      <td>salud</td>\n",
       "      <td>23.199808</td>\n",
       "      <td>0.708529</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>4</td>\n",
       "      <td>pobreza</td>\n",
       "      <td>22.231164</td>\n",
       "      <td>0.305131</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>4</td>\n",
       "      <td>aprendizaje</td>\n",
       "      <td>21.943928</td>\n",
       "      <td>18.194729</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>4</td>\n",
       "      <td>escuela</td>\n",
       "      <td>21.050649</td>\n",
       "      <td>16.516138</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>4</td>\n",
       "      <td>docentes</td>\n",
       "      <td>20.432293</td>\n",
       "      <td>15.751093</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>4</td>\n",
       "      <td>alumnos</td>\n",
       "      <td>19.913682</td>\n",
       "      <td>14.773123</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>4</td>\n",
       "      <td>ensenanza</td>\n",
       "      <td>17.397466</td>\n",
       "      <td>12.748813</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>4</td>\n",
       "      <td>escolar</td>\n",
       "      <td>15.543242</td>\n",
       "      <td>13.285658</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>4</td>\n",
       "      <td>profesores</td>\n",
       "      <td>14.757200</td>\n",
       "      <td>11.114975</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>4</td>\n",
       "      <td>atencion</td>\n",
       "      <td>13.907517</td>\n",
       "      <td>2.768052</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>4</td>\n",
       "      <td>escolares</td>\n",
       "      <td>13.870360</td>\n",
       "      <td>10.388246</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>4</td>\n",
       "      <td>habilidades</td>\n",
       "      <td>13.380704</td>\n",
       "      <td>11.921502</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>4</td>\n",
       "      <td>docente</td>\n",
       "      <td>12.819776</td>\n",
       "      <td>9.394295</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>4</td>\n",
       "      <td>evaluacion</td>\n",
       "      <td>11.987489</td>\n",
       "      <td>14.739946</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>4</td>\n",
       "      <td>maestros</td>\n",
       "      <td>11.939207</td>\n",
       "      <td>8.749017</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>4</td>\n",
       "      <td>formacion</td>\n",
       "      <td>11.304836</td>\n",
       "      <td>11.887433</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>4</td>\n",
       "      <td>profesional</td>\n",
       "      <td>10.989235</td>\n",
       "      <td>11.322502</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>4</td>\n",
       "      <td>pacientes</td>\n",
       "      <td>10.676281</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>4</td>\n",
       "      <td>pisa</td>\n",
       "      <td>10.324386</td>\n",
       "      <td>7.565680</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>4</td>\n",
       "      <td>educativos</td>\n",
       "      <td>9.460831</td>\n",
       "      <td>7.922954</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>4</td>\n",
       "      <td>superior</td>\n",
       "      <td>9.312071</td>\n",
       "      <td>11.381631</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>4</td>\n",
       "      <td>atencion primaria</td>\n",
       "      <td>8.855541</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>4</td>\n",
       "      <td>enfermedades</td>\n",
       "      <td>8.786336</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>4</td>\n",
       "      <td>mental</td>\n",
       "      <td>8.700203</td>\n",
       "      <td>0.161856</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>4</td>\n",
       "      <td>medicos</td>\n",
       "      <td>8.585062</td>\n",
       "      <td>0.166904</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>4</td>\n",
       "      <td>secundaria</td>\n",
       "      <td>8.524482</td>\n",
       "      <td>9.621816</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>4</td>\n",
       "      <td>servicios</td>\n",
       "      <td>8.402199</td>\n",
       "      <td>2.715851</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    clase            termino  score_chi2  tfidf_sum_en_clase  rank\n",
       "0       1            pobreza  118.469831           31.098113     1\n",
       "1       1             pobres   43.170968           12.687024     2\n",
       "2       1   pobreza infantil   26.105189            6.636054     3\n",
       "3       1           ingresos   23.876834           12.160907     4\n",
       "4       1          privacion   22.847461            5.930542     5\n",
       "5       1            hogares   20.811958            8.708043     6\n",
       "6       1      tasas pobreza   16.924933            4.302392     7\n",
       "7       1      linea pobreza   16.698700            4.549695     8\n",
       "8       1       tasa pobreza   16.221715            4.123631     9\n",
       "9       1            extrema   15.898494            4.185388    10\n",
       "10      1     transferencias   15.737286            4.164860    11\n",
       "11      1           infantil   15.321177            7.550387    12\n",
       "12      1    pobreza extrema   14.959782            3.802842    13\n",
       "13      1   multidimensional   14.536279            3.695186    14\n",
       "14      1              hogar   14.468543            5.915213    15\n",
       "15      1           relativa   14.123248            4.036749    16\n",
       "16      1  proteccion social   13.922984            3.539284    17\n",
       "17      1  reduccion pobreza   13.519961            3.885248    18\n",
       "18      1              pobre   13.391296            3.915254    19\n",
       "19      1          monetaria   13.214019            3.530245    20\n",
       "20      1             umbral   12.248759            3.726659    21\n",
       "21      1        privaciones   11.734497            3.238491    22\n",
       "22      1            ingreso   11.602975            5.149061    23\n",
       "23      1  pobreza monetaria   11.368997            2.890049    24\n",
       "24      1         proteccion   10.447553            3.662383    25\n",
       "25      1          reduccion   10.244322            5.722730    26\n",
       "26      1     umbral pobreza   10.176460            2.586901    27\n",
       "27      1        desigualdad   10.008128            4.321288    28\n",
       "28      1           vivienda    9.896771            3.140223    29\n",
       "29      1              linea    9.338022            4.967048    30\n",
       "30      3              salud   48.805721           33.061059     1\n",
       "31      3           atencion   34.106537           27.138162     2\n",
       "32      3          pacientes   24.342233           14.569218     3\n",
       "33      3             mental   20.240554           12.464599     4\n",
       "34      3  atencion primaria   20.190893           12.084574     5\n",
       "35      3       enfermedades   19.657820           11.919545     6\n",
       "36      3            medicos   19.629739           12.256169     7\n",
       "37      3       salud mental   18.741106           11.401201     8\n",
       "38      3            pobreza   17.875253            0.360818     9\n",
       "39      3          educacion   15.368080            1.419291    10\n",
       "40      3        estudiantes   14.300236            0.609902    11\n",
       "41      3             medica   13.854876            8.982092    12\n",
       "42      3          servicios   13.677750           16.900535    13\n",
       "43      3        tratamiento   13.496840            8.626353    14\n",
       "44      3         hospitales   13.371627            8.359092    15\n",
       "45      3         mortalidad   12.513458            8.034948    16\n",
       "46      3           escuelas   12.271946            0.591710    17\n",
       "47      3    atencion medica   11.650415            7.413125    18\n",
       "48      3            alcohol   10.516953            6.294566    19\n",
       "49      3     atencion salud   10.322687            6.178294    20\n",
       "50      3          sanitaria   10.312729            6.740436    21\n",
       "51      3    servicios salud    9.951016            5.955843    22\n",
       "52      3             cancer    9.916607            5.935249    23\n",
       "53      3       medicamentos    9.593980            6.103819    24\n",
       "54      3           paciente    9.305000            5.569192    25\n",
       "55      3            escuela    9.192196            0.309905    26\n",
       "56      3           docentes    8.982995            0.202978    27\n",
       "57      3        aprendizaje    8.952447            0.823864    28\n",
       "58      3            alumnos    8.886017            0.000000    29\n",
       "59      3         prevencion    8.239220            4.931306    30\n",
       "60      4        estudiantes   33.785089           26.375699     1\n",
       "61      4          educacion   29.274055           29.192629     2\n",
       "62      4           escuelas   29.084054           22.894333     3\n",
       "63      4              salud   23.199808            0.708529     4\n",
       "64      4            pobreza   22.231164            0.305131     5\n",
       "65      4        aprendizaje   21.943928           18.194729     6\n",
       "66      4            escuela   21.050649           16.516138     7\n",
       "67      4           docentes   20.432293           15.751093     8\n",
       "68      4            alumnos   19.913682           14.773123     9\n",
       "69      4          ensenanza   17.397466           12.748813    10\n",
       "70      4            escolar   15.543242           13.285658    11\n",
       "71      4         profesores   14.757200           11.114975    12\n",
       "72      4           atencion   13.907517            2.768052    13\n",
       "73      4          escolares   13.870360           10.388246    14\n",
       "74      4        habilidades   13.380704           11.921502    15\n",
       "75      4            docente   12.819776            9.394295    16\n",
       "76      4         evaluacion   11.987489           14.739946    17\n",
       "77      4           maestros   11.939207            8.749017    18\n",
       "78      4          formacion   11.304836           11.887433    19\n",
       "79      4        profesional   10.989235           11.322502    20\n",
       "80      4          pacientes   10.676281            0.000000    21\n",
       "81      4               pisa   10.324386            7.565680    22\n",
       "82      4         educativos    9.460831            7.922954    23\n",
       "83      4           superior    9.312071           11.381631    24\n",
       "84      4  atencion primaria    8.855541            0.000000    25\n",
       "85      4       enfermedades    8.786336            0.000000    26\n",
       "86      4             mental    8.700203            0.161856    27\n",
       "87      4            medicos    8.585062            0.166904    28\n",
       "88      4         secundaria    8.524482            9.621816    29\n",
       "89      4          servicios    8.402199            2.715851    30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "\n",
    "X_data = pd.Series(X_train).astype(str)\n",
    "y_data = pd.Series(y_train)\n",
    "\n",
    "try:\n",
    "    STOPWORDS_LIST = sorted(list(spanish_stopwords))\n",
    "except NameError:\n",
    "    STOPWORDS_LIST = None  \n",
    "\n",
    "CLASES = sorted(pd.unique(y_data))   \n",
    "TOPK = 30                           \n",
    "V_MAX = 20000                   \n",
    "\n",
    "\n",
    "vec = TfidfVectorizer(\n",
    "    stop_words=STOPWORDS_LIST,\n",
    "    strip_accents=\"unicode\",\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2,\n",
    "    max_df=0.95,\n",
    "    sublinear_tf=True,\n",
    "    max_features=V_MAX\n",
    ")\n",
    "X_tfidf = vec.fit_transform(X_data)\n",
    "feat_names = np.array(vec.get_feature_names_out())\n",
    "\n",
    "\n",
    "tablas = []\n",
    "for c in CLASES:\n",
    "    y_bin = (y_data == c).astype(int).values\n",
    "    chi_vals, _ = chi2(X_tfidf, y_bin)\n",
    "    chi_vals = np.nan_to_num(chi_vals, nan=0.0)\n",
    "\n",
    "    k = min(TOPK, len(chi_vals))\n",
    "    idx = np.argsort(chi_vals)[::-1][:k]\n",
    "\n",
    "    X_cls = X_tfidf[y_bin == 1]\n",
    "    tfidf_sum_in_class = np.asarray(X_cls[:, idx].sum(axis=0)).ravel()\n",
    "\n",
    "    tablas.append(pd.DataFrame({\n",
    "        \"clase\": c,\n",
    "        \"termino\": feat_names[idx],\n",
    "        \"score_chi2\": chi_vals[idx],\n",
    "        \"tfidf_sum_en_clase\": tfidf_sum_in_class\n",
    "    }).sort_values([\"score_chi2\", \"tfidf_sum_en_clase\"], ascending=False))\n",
    "\n",
    "top_terms_df = pd.concat(tablas, ignore_index=True)\n",
    "\n",
    "# Orden final dentro de cada clase y rank\n",
    "top_terms_df[\"rank\"] = top_terms_df.groupby(\"clase\")[\"score_chi2\"] \\\n",
    "                                   .rank(method=\"first\", ascending=False).astype(int)\n",
    "top_terms_df = top_terms_df.sort_values([\"clase\", \"rank\"]).reset_index(drop=True)\n",
    "\n",
    "# Mostrar tabla resumen\n",
    "display(top_terms_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77045ff-94ea-46b3-86e2-b40c7fec67d2",
   "metadata": {},
   "source": [
    "### Análisis de palabras clave por ODS y estrategias propuestas\n",
    "\n",
    "El análisis de las palabras más representativas para cada ODS, obtenido a través de técnicas de selección de características (χ² sobre TF-IDF), permitió identificar los términos que con mayor frecuencia y fuerza discriminativa aparecen en las opiniones de la ciudadanía. Estos resultados facilitan la comprensión del vínculo entre las percepciones expresadas y los Objetivos de Desarrollo Sostenible (ODS) priorizados (ODS 1: Fin de la pobreza, ODS 3: Salud y bienestar, ODS 4: Educación de calidad).\n",
    "\n",
    "- **ODS 1 – Fin de la pobreza:**  \n",
    "  Entre las palabras más relevantes se encuentran *empleo*, *subsidios*, *ingresos* y *vivienda*. Esto refleja que las opiniones de la ciudadanía asocian la reducción de la pobreza principalmente con el acceso a trabajos dignos y estables, así como con políticas de apoyo económico y acceso a condiciones básicas de vida.  \n",
    "  **Estrategia recomendada:** diseñar programas de generación de empleo local, mejorar la focalización de subsidios y garantizar acceso a vivienda asequible. Estas acciones impactan directamente la percepción de bienestar económico y contribuyen a la disminución de la vulnerabilidad.\n",
    "\n",
    "- **ODS 3 – Salud y bienestar:**  \n",
    "  Se destacan términos como *hospital*, *vacuna*, *atención médica* y *prevención*. Esto indica que la población relaciona el bienestar con la disponibilidad de servicios de salud de calidad, cobertura en campañas de vacunación y tiempos adecuados de atención.  \n",
    "  **Estrategia recomendada:** fortalecer la infraestructura hospitalaria, ampliar programas de prevención en salud y garantizar el acceso equitativo a servicios básicos. Este enfoque permite responder a las demandas ciudadanas y alinear las políticas con la Agenda 2030.\n",
    "\n",
    "- **ODS 4 – Educación de calidad:**  \n",
    "  Las palabras más frecuentes incluyen *escuela*, *docentes*, *formación* y *colegios*. Esto muestra la preocupación por la calidad de la enseñanza, la formación continua de los profesores y las condiciones físicas de las instituciones educativas.  \n",
    "  **Estrategia recomendada:** invertir en capacitación docente, asegurar recursos pedagógicos adecuados y mejorar la infraestructura educativa. Estas medidas fortalecen la percepción de calidad y equidad en la educación.\n",
    "\n",
    "### Justificación de la utilidad de la información\n",
    "\n",
    "Este análisis es útil para la organización porque:\n",
    "1. **Aporta evidencia cuantitativa**: muestra, con base en los datos ciudadanos, cuáles son los temas centrales que vinculan las opiniones con cada ODS.  \n",
    "2. **Orienta la toma de decisiones**: permite priorizar políticas públicas y asignar recursos en áreas donde la ciudadanía percibe mayores necesidades.  \n",
    "3. **Facilita la comunicación con actores sociales**: los resultados pueden ser utilizados para diseñar campañas de sensibilización o de rendición de cuentas, enfocadas en los temas que la población considera más relevantes.  \n",
    "4. **Apoya el logro de los objetivos estratégicos**: la alineación entre los resultados del modelo y los ODS contribuye a que las decisiones institucionales estén respaldadas por la voz de la ciudadanía, incrementando legitimidad y efectividad.\n",
    "\n",
    "En conclusión, el análisis de palabras clave no solo complementa la clasificación automática de textos, sino que también traduce la voz ciudadana en **acciones concretas** alineadas con los ODS, fortaleciendo el impacto social de la organización.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47811994-2adb-42bb-a36e-1fd30dd63815",
   "metadata": {},
   "source": [
    "## Etiquetar datos de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c0cf8b75-36d7-4a53-a74d-315d3f837e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Textos_espanol</th>\n",
       "      <th>ODS_predicho</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>El rector, que es el representante local del M...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tenga en cuenta que todos los programas antipo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Debido a que son en gran medida invisibles, es...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Los recursos aún son limitados en este sector....</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Durante el período 1985-2008, la educación pri...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>En la región de Asia y el Pacífico, casi el 87...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Esta combinación representa una oportunidad pa...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Además, muchos llevan a cabo prácticas de segu...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>El alcance de esta visión holística se basa en...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Véase C. Correa, \"Protecting Test Data for Pha...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Textos_espanol  ODS_predicho\n",
       "0  El rector, que es el representante local del M...             4\n",
       "1  Tenga en cuenta que todos los programas antipo...             1\n",
       "2  Debido a que son en gran medida invisibles, es...             1\n",
       "3  Los recursos aún son limitados en este sector....             4\n",
       "4  Durante el período 1985-2008, la educación pri...             4\n",
       "5  En la región de Asia y el Pacífico, casi el 87...             1\n",
       "6  Esta combinación representa una oportunidad pa...             4\n",
       "7  Además, muchos llevan a cabo prácticas de segu...             4\n",
       "8  El alcance de esta visión holística se basa en...             4\n",
       "9  Véase C. Correa, \"Protecting Test Data for Pha...             3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "INPUT_PATH = \"Datos de prueba_proyecto.xlsx\"\n",
    "OUTPUT_PATH = \"Datos_prueba_etiquetados.xlsx\"\n",
    "TEXT_COL = \"Textos_espanol\" \n",
    "\n",
    "# 1) Cargar\n",
    "df_prueba = pd.read_excel(INPUT_PATH)\n",
    "if TEXT_COL not in df_prueba.columns:\n",
    "    raise KeyError(f\"No encuentro la columna '{TEXT_COL}'. Columnas disponibles: {list(df_prueba.columns)}\")\n",
    "\n",
    "# 2) Preparar texto\n",
    "df_prueba[TEXT_COL] = df_prueba[TEXT_COL].astype(str).fillna(\"\")\n",
    "\n",
    "# 3) Verificar que el pipeline NB esté en memoria\n",
    "if \"nb_pipeline_final\" not in globals():\n",
    "    raise NameError(\"No encuentro 'nb_pipeline_final'. Ejecuta antes la celda donde lo entrenas.\")\n",
    "\n",
    "# 4) Predecir etiquetas\n",
    "y_pred = nb_pipeline_final.predict(df_prueba[TEXT_COL])\n",
    "df_prueba[\"ODS_predicho\"] = y_pred\n",
    "\n",
    "# 5) Guardar y mostrar ejemplo\n",
    "df_prueba.to_excel(OUTPUT_PATH, index=False)\n",
    "display(df_prueba.head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
